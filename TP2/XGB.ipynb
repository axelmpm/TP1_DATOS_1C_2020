{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "%run Functions.ipynb\n",
    "%run ../datuslib.ipynb\n",
    "import pandas as pd\n",
    "import string\n",
    "import numpy as np\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import time\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CARGA DATA\n",
    "#negativity = pd.read_csv(\"../Axel/tweets_con_negatividad.csv\")\n",
    "train = pd.read_csv(\"Data/train.csv\")\n",
    "#train[\"negativity\"] = negativity[\"negativity\"]\n",
    "\n",
    "to_predict = pd.read_csv(\"Data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features y Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hal9000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train.fillna(\"Nothing\", inplace= True)\n",
    "features(train, to_predict)\n",
    "keyword_in_text(train)\n",
    "\n",
    "train.drop(columns=[\"id\"], axis=1,inplace = True)\n",
    "\n",
    "train.drop(columns=[\"location\"], axis=1,inplace = True)\n",
    "\n",
    "train['keyword'] = train['keyword'].str.replace('%',' ')\n",
    "train['keyword'] = train['keyword'].str.replace('2','')\n",
    "train['keyword'] = train['keyword'].str.replace('0','')\n",
    "train.drop(columns=[\"text\"], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"Tiene_key_impor\"] = (train[\"keyword\"] == \"wreckage\") | (train[\"keyword\"] == \"debris\") | (train[\"keyword\"] == \"derailment\") \n",
    "#| (train[\"keyword\"] == \"outbreak\") | (train[\"keyword\"] == \"oil split\")\\\n",
    "#| (train[\"keyword\"] == \"typhoon\") | (train[\"keyword\"] == \"suicide bombing\") | (train[\"keyword\"] == \"suicide bomber\")\n",
    "\n",
    "train[\"Tiene_key_no_impor\"] = (train[\"keyword\"] == \"aftershock\")\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = train.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = train.columns[categorical_feature_mask].tolist()\n",
    "categorical_cols\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply le on categorical feature columns\n",
    "train[categorical_cols] = train[categorical_cols].apply(lambda col: le.fit_transform(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shapes: X=(7613, 11) y=(7613,)\n",
      "Train shapes: X=(6090, 11) y=(6090,)\n",
      "Test  shapes: X=(1523, 11)  y=(1523,)\n"
     ]
    }
   ],
   "source": [
    "x = train[train.columns.drop(\"target\")]\n",
    "y = train['target']\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.20)\n",
    "\n",
    "print(f\"Original shapes: X={x.shape} y={y.shape}\")\n",
    "print(f\"Train shapes: X={x_train.shape} y={y_train.shape}\")\n",
    "print(f\"Test  shapes: X={x_test.shape}  y={y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.5, gamma=1,\n",
       "              learning_rate=0.02, max_delta_step=0, max_depth=40,\n",
       "              min_child_weight=1, missing=None, n_estimators=900, n_jobs=6,\n",
       "              nthread=None, objective='binary:logistic', random_state=0,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "              silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier(n_estimators=900,n_jobs=6,objective=\"binary:logistic\",\n",
    "                     max_depth= 40,min_child_weight = 1,colsample_bytree=0.5,\n",
    "                     gamma=1, learning_rate=0.02, subsample=1)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6.5664873123168945 seconds ---\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entrenamiento: 95.4351%\n",
      "Testeo: 75.7058%.\n",
      "F1 score: 0.7571.\n",
      "   \n"
     ]
    }
   ],
   "source": [
    "predecir(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predecir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Completo Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hal9000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "to_predict.fillna(\"Nothing\", inplace= True)\n",
    "keyword_in_text(to_predict)\n",
    "\n",
    "ids = to_predict[\"id\"]\n",
    "\n",
    "to_predict.drop(columns=[\"id\"], axis=1,inplace = True)\n",
    "to_predict.drop(columns=[\"location\"], axis=1,inplace = True)\n",
    "\n",
    "to_predict['keyword'] = to_predict['keyword'].str.replace('%',' ')\n",
    "to_predict['keyword'] = to_predict['keyword'].str.replace('2','')\n",
    "to_predict['keyword'] = to_predict['keyword'].str.replace('0','')\n",
    "\n",
    "#importance(to_predict)\n",
    "to_predict.drop(columns=[\"text\"], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_predict[\"Tiene_key_impor\"] = (to_predict[\"keyword\"] == \"wreckage\") | (to_predict[\"keyword\"] == \"debris\")\\\n",
    "| (to_predict[\"keyword\"] == \"derailment\")\n",
    "#| (to_predict[\"keyword\"] == \"outbreak\")\\\n",
    "#| (to_predict[\"keyword\"] == \"oil split\") | (to_predict[\"keyword\"] == \"typhoon\") \\\n",
    "#| (to_predict[\"keyword\"] == \"suicide bombing\") | (to_predict[\"keyword\"] == \"suicide bomber\")\n",
    "\n",
    "to_predict[\"Tiene_key_no_impor\"] = (to_predict[\"keyword\"] == \"aftershock\")\n",
    "\n",
    "# Categorical boolean mask\n",
    "categorical_feature_mask = to_predict.dtypes==object\n",
    "# filter categorical columns using mask and turn it into a list\n",
    "categorical_cols = to_predict.columns[categorical_feature_mask].tolist()\n",
    "categorical_cols\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "# apply le on categorical feature columns\n",
    "to_predict[categorical_cols] = to_predict[categorical_cols].apply(lambda col: le.fit_transform(col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predict = model.predict( to_predict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compruebo el F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.6969046889365614\n"
     ]
    }
   ],
   "source": [
    "comprobar(best_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TUNNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "param_test = {\n",
    "    'n_estimators':range(100, 1000, 100),   \n",
    "    'learning_rate':(0.01,0.02,0.03,0.04,0.05,0.06,0.07,0.08,0.09,0.1)\n",
    "   # 'subsample':[0.7,1],\n",
    "   # 'gamma':[1,15,30],\n",
    "   # 'colsample_bytree':[0.5,1.0],\n",
    "  #  'max_depth':range(5,50,5)\n",
    "  #  'min_child_weight' : range(1,10,1) \n",
    "}\n",
    "\n",
    "search = GridSearchCV(\n",
    "        estimator = XGBClassifier(max_depth = 40,objective=\"binary:logistic\",colsample_bytree=0.5, gamma=1, subsample=1),\n",
    "        param_grid = param_test,\n",
    "        n_jobs=4,\n",
    "        cv=3,\n",
    "        verbose=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 90 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    5.2s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    9.5s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:   20.7s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   36.6s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   45.4s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   57.1s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed:  3.6min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed:  5.1min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=4)]: Done 270 out of 270 | elapsed:  7.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score='raise-deprecating',\n",
       "             estimator=XGBClassifier(base_score=0.5, booster='gbtree',\n",
       "                                     colsample_bylevel=1, colsample_bynode=1,\n",
       "                                     colsample_bytree=0.5, gamma=1,\n",
       "                                     learning_rate=0.1, max_delta_step=0,\n",
       "                                     max_depth=40, min_child_weight=1,\n",
       "                                     missing=None, n_estimators=100, n_jobs=1,\n",
       "                                     nthread=None, objective='binary:logistic',\n",
       "                                     random_state=0, reg_alpha=0, reg_lambda=1,\n",
       "                                     scale_pos_weight=1, seed=None, silent=None,\n",
       "                                     subsample=1, verbosity=1),\n",
       "             iid='warn', n_jobs=4,\n",
       "             param_grid={'learning_rate': (0.01, 0.02, 0.03, 0.04, 0.05, 0.06,\n",
       "                                           0.07, 0.08, 0.09, 0.1),\n",
       "                         'n_estimators': range(100, 1000, 100)},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=10)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'learning_rate': 0.01, 'n_estimators': 300}, 0.7279146141215107)"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.best_params_, search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_predict = model.predict( to_predict )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7174379405455102\n"
     ]
    }
   ],
   "source": [
    "comprobar(best_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
