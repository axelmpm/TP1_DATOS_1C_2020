{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def submission_output(ids, prediction,name):\n",
    "    df_ids = ids\n",
    "    result = df_ids.to_frame()\n",
    "    result[\"target\"] = prediction\n",
    "    result.to_csv('predictions/'+name, index=False)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir(model, train_features, train_labels, test_features, test_labels):\n",
    "    score = model.score(test_features,test_labels)\n",
    "    predict = model.predict(test_features)\n",
    "    \n",
    "    print('Entrenamiento: {:0.4f}%'.format(model.score(train_features, train_labels)*100))\n",
    "    print('Testeo: {:0.4f}%.'.format(score*100))\n",
    "    \n",
    "    error1 = f1_score(test_labels, predict, average='micro')\n",
    "    print('F1 score: {:0.4f}.'.format(error1))\n",
    "    print (\"   \")\n",
    "    return error1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(df_train, df_test):\n",
    "    df_test['len']=df_test.text.map(lambda x: len(x)/280)\n",
    "    df_test[\"word_count\"] = df_test.text.map(lambda x: len(x.split(\" \")))\n",
    "    df_test[\"word_count\"] = df_test[\"word_count\"]/df_test[\"word_count\"].max()\n",
    "    #df_test['importance']= df_test.text.map(lambda x: calculateTweetImportance(x))\n",
    "    \n",
    "    df_train['len']=df_train.text.map(lambda x: len(x)/280)\n",
    "    df_train[\"word_count\"] = df_train.text.map(lambda x: len(x.split(\" \")))\n",
    "    df_train[\"word_count\"] = df_train[\"word_count\"]/df_train[\"word_count\"].max()\n",
    "    #df_train['importance']= df_train.text.map(lambda x: calculateTweetImportance(x))\n",
    "    \n",
    "    #df_train['word_count'] = df_train['text'].apply(lambda x: len(str(x).split()))\n",
    "    #df_test['word_count'] = df_test['text'].apply(lambda x: len(str(x).split()))\n",
    "\n",
    "    # unique_word_count\n",
    "    #df_train['unique_word_count'] = df_train['text'].apply(lambda x: len(set(str(x).split())))\n",
    "    #df_test['unique_word_count'] = df_test['text'].apply(lambda x: len(set(str(x).split())))\n",
    "\n",
    "    # stop_word_count\n",
    "    #df_train['stop_word_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "    #df_test['stop_word_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n",
    "\n",
    "    # url_count\n",
    "    #df_train['url_count'] = df_train['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "    #df_test['url_count'] = df_test['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n",
    "    \n",
    "    #df_train[\"url_count\"] = df_train[\"url_count\"] > 0\n",
    "    #df_test[\"url_count\"] = df_test[\"url_count\"] > 0\n",
    "\n",
    "    # mean_word_length\n",
    "   # df_train['mean_word_length'] = df_train['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "   # df_test['mean_word_length'] = df_test['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n",
    "\n",
    "    # punctuation_count\n",
    "    #df_train['punctuation_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    #df_test['punctuation_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n",
    "    #df_train[\"punctuation_count\"] = df_train[\"punctuation_count\"] > 0\n",
    "    #df_test[\"punctuation_count\"] = df_test[\"punctuation_count\"] > 0\n",
    "    # hashtag_count\n",
    "    #df_train['hashtag_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "    #df_test['hashtag_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n",
    "\n",
    "    #mention_count\n",
    "    #df_train['mention_count'] = df_train['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n",
    "    #df_test['mention_count'] = df_test['text'].apply(lambda x: len([c for c in str(x) if c == '@'])) \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_in_text(df):\n",
    "    \"\"\"\n",
    "    RECIBE: un df\n",
    "    DEVUELVE: el mismo df con una columna que indica si la keyword esta en el tweet.\n",
    "    \"\"\"\n",
    "    \n",
    "    df['contains_keyword'] = 0\n",
    "    cant_filas = len(df.index)\n",
    "    for y in range(cant_filas):\n",
    "        df['contains_keyword'][y:]= df.iloc[y][1].lower() in df.iloc[y][3].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_best_distance(df,dist,x_train,y_train,x_test,y_test):\n",
    "    for x in dist:\n",
    "        print(x+\" --------------------------\")\n",
    "        result , k = knn_predictor(x,x_train,y_train,x_test,y_test)\n",
    "        \n",
    "        df[x] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_predictor(distancia,x_train,y_train,x_test,y_test):\n",
    "    kesimos = []\n",
    "    result = []\n",
    "    for k in range(2,40):\n",
    "        print(\"con k: \", k)\n",
    "        kesimos.append(k)\n",
    "        knn = KNeighborsClassifier(n_neighbors = k, metric=distancia)\n",
    "        knn.fit(x_train, y_train)\n",
    "        res = predecir(knn,x_train,y_train,x_test,y_test)\n",
    "        result.append(res)\n",
    "    return result, kesimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comprobar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicciones Reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comprobar(best_predict):\n",
    "    test_labels = pd.read_csv(\"Data/perfect_submission.csv\")\n",
    "    test_labels = test_labels[\"target\"]\n",
    "    print (\"F1 Score: \", f1_score(test_labels, best_predict,average='micro'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
