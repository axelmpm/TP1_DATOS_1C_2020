{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import time\n",
    "import category_encoders as ce\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "%run Functions.ipynb\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report,accuracy_score\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer, Binarizer,LabelEncoder, LabelBinarizer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegressionCV, Perceptron, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from category_encoders import TargetEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Extracting data from .csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_train = pd.read_csv('train_features_1.csv')\n",
    "key_test = pd.read_csv('test_features_1.csv')\n",
    "\n",
    "#Uso train y test filtrado por axel para poder usar datos de location\n",
    "train_tweets = pd.read_csv('FE_1_train.csv')\n",
    "test_tweets = pd.read_csv('FE_1_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dict_from_file(name):\n",
    "    f = open(name,'r')\n",
    "    data=f.read()\n",
    "    f.close()\n",
    "    return eval(data)\n",
    "\n",
    "dict_0 = load_dict_from_file(\"dict_0.txt\")\n",
    "dict_1 = load_dict_from_file(\"dict_1.txt\")\n",
    "listaPalabras = []\n",
    "\n",
    "archivo = open(\"NW.txt\", \"r\")\n",
    "for linea in archivo.readlines():\n",
    "    linea = linea.rstrip('\\n') \n",
    "    listaPalabras.append(linea)\n",
    "archivo.close()\n",
    "\n",
    "\n",
    "\n",
    "prob_words(train_tweets,test_tweets,listaPalabras,dict_0,dict_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hal9000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "train_tweets[\"word_count\"] =key_train[\"word_count\"]\n",
    "train_tweets[\"unique_word_count\"] =key_train[\"unique_word_count\"]\n",
    "train_tweets[\"url_count\"] = key_train[\"url_count\"]\n",
    "train_tweets[\"mean_word_length\"] = key_train[\"mean_word_length\"]\n",
    "\n",
    "train_tweets\\\n",
    ".drop(columns=[\"id\",'complete_text','negativity','sigmoidic_negativity','neg_to_label','Unnamed: 0','location'], axis=1,inplace = True)\n",
    "\n",
    "test_tweets[\"word_count\"] =key_test[\"word_count\"]\n",
    "test_tweets[\"unique_word_count\"] =key_test[\"unique_word_count\"]\n",
    "test_tweets[\"url_count\"] = key_test[\"url_count\"]\n",
    "test_tweets[\"mean_word_length\"] = key_test[\"mean_word_length\"]\n",
    "\n",
    "ids = test_tweets[\"id\"]\n",
    "test_tweets\\\n",
    ".drop(columns=[\"id\",'complete_text','negativity','sigmoidic_negativity','neg_to_label','Unnamed: 0','location'], axis=1,inplace = True)\n",
    "\n",
    "train_tweets[\"keyword\"] = key_train[\"keyword\"]\n",
    "test_tweets[\"keyword\"] = key_test[\"keyword\"]\n",
    "train_tweets[\"text_filtrado\"] = key_train[\"text\"]\n",
    "test_tweets[\"text_filtrado\"] = key_test[\"text\"]\n",
    "\n",
    "train_tweets.fillna(\"NaN\", inplace= True)\n",
    "test_tweets.fillna(\"NaN\", inplace= True)\n",
    "train_tweets['keyword'] = train_tweets['keyword'].str.replace('%',' ')\n",
    "train_tweets['keyword'] = train_tweets['keyword'].str.replace('2','')\n",
    "train_tweets['keyword'] = train_tweets['keyword'].str.replace('0','')\n",
    "\n",
    "test_tweets['keyword'] = test_tweets['keyword'].str.replace('%',' ')\n",
    "test_tweets['keyword'] = test_tweets['keyword'].str.replace('2','')\n",
    "test_tweets['keyword'] = test_tweets['keyword'].str.replace('0','')\n",
    "\n",
    "keyword_in_text_2(train_tweets)\n",
    "keyword_in_text_2(test_tweets)\n",
    "Has(train_tweets)\n",
    "Has(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import category_encoders as ce\n",
    "\n",
    "ct =ce.TargetEncoder()\n",
    "train_tweets[\"key_endoder\"]= ct.fit_transform(train_tweets[\"keyword\"],train_tweets[\"target\"])\n",
    "test_tweets[\"key_endoder\"]= ct.transform(test_tweets[\"keyword\"])\n",
    "\n",
    "le = LabelEncoder()\n",
    "train_tweets[\"Label_key\"] = le.fit_transform(train_tweets[\"keyword\"])\n",
    "test_tweets[\"Label_key\"] = le.transform(test_tweets[\"keyword\"])\n",
    "\n",
    "y = train_tweets['target']\n",
    "\n",
    "train_tweets.drop(columns=['target'], axis=1,inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bi = ce.BinaryEncoder(cols = [\"continent\",\"country\",\"city\",\"keyword\"],handle_unknown=\"value\")\n",
    "train_tweets = bi.fit_transform(train_tweets)\n",
    "test_tweets = bi.transform(test_tweets)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "oh=ce.OneHotEncoder(cols =[\"Label_key\"],handle_unknown=\"ignore\")\n",
    "train_tweets = oh.fit_transform(train_tweets)\n",
    "test_tweets = oh.transform(test_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate(number, digits) -> float:\n",
    "    stepper = 10.0 ** digits\n",
    "    return math.trunc(stepper * number) / stepper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets[\"mean_word_length\"] =train_tweets[\"mean_word_length\"].map(lambda x: truncate(x,3))\n",
    "test_tweets[\"mean_word_length\"] =test_tweets[\"mean_word_length\"].map(lambda x: truncate(x,3))\n",
    "#train_tweets[\"Puntaje_ser_1\"] =train_tweets[\"Puntaje_ser_1\"].map(lambda x: truncate(x,3))\n",
    "#test_tweets[\"Puntaje_ser_0\"] =test_tweets[\"Puntaje_ser_0\"].map(lambda x: truncate(x,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_0</th>\n",
       "      <th>keyword_1</th>\n",
       "      <th>keyword_2</th>\n",
       "      <th>keyword_3</th>\n",
       "      <th>keyword_4</th>\n",
       "      <th>keyword_5</th>\n",
       "      <th>keyword_6</th>\n",
       "      <th>keyword_7</th>\n",
       "      <th>keyword_8</th>\n",
       "      <th>text</th>\n",
       "      <th>continent_0</th>\n",
       "      <th>continent_1</th>\n",
       "      <th>continent_2</th>\n",
       "      <th>continent_3</th>\n",
       "      <th>continent_4</th>\n",
       "      <th>country_0</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>country_3</th>\n",
       "      <th>country_4</th>\n",
       "      <th>country_5</th>\n",
       "      <th>country_6</th>\n",
       "      <th>country_7</th>\n",
       "      <th>city_0</th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>city_3</th>\n",
       "      <th>city_4</th>\n",
       "      <th>city_5</th>\n",
       "      <th>city_6</th>\n",
       "      <th>city_7</th>\n",
       "      <th>city_8</th>\n",
       "      <th>city_9</th>\n",
       "      <th>city_10</th>\n",
       "      <th>hasNum</th>\n",
       "      <th>hasLink</th>\n",
       "      <th>hasUser</th>\n",
       "      <th>hasHashtag</th>\n",
       "      <th>exclamacion</th>\n",
       "      <th>interrogacion</th>\n",
       "      <th>mayusculas</th>\n",
       "      <th>longitud</th>\n",
       "      <th>Puntaje_ser_1</th>\n",
       "      <th>Puntaje_ser_0</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>text_filtrado</th>\n",
       "      <th>contains_keyword</th>\n",
       "      <th>Tiene_key_impor</th>\n",
       "      <th>Tiene_key_no_impor</th>\n",
       "      <th>key_endoder</th>\n",
       "      <th>Label_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>71</td>\n",
       "      <td>1.512000</td>\n",
       "      <td>2.294000</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>4.384</td>\n",
       "      <td>our deeds are reason earthquake may allah forg...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>1.697714</td>\n",
       "      <td>2.108286</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4.571</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>135</td>\n",
       "      <td>1.612000</td>\n",
       "      <td>2.194000</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>5.090</td>\n",
       "      <td>residents asked shelter in place are being not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>2.966851</td>\n",
       "      <td>4.839149</td>\n",
       "      <td>9</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7.125</td>\n",
       "      <td>people receive wildfires evacuation orders in ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>90</td>\n",
       "      <td>2.766851</td>\n",
       "      <td>4.039149</td>\n",
       "      <td>17</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>4.500</td>\n",
       "      <td>just got sent photo from ruby alaska smoke fro...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Two giant cranes holding a bridge collapse int...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>85</td>\n",
       "      <td>1.732000</td>\n",
       "      <td>2.074000</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>6.636</td>\n",
       "      <td>two giant cranes holding bridge collapse into ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@aria_ahrary @TheTawniest The out of control w...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>127</td>\n",
       "      <td>1.857714</td>\n",
       "      <td>2.948286</td>\n",
       "      <td>20</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>5.300</td>\n",
       "      <td>ariaahrary thetawniest out control wild fires ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>67</td>\n",
       "      <td>1.412000</td>\n",
       "      <td>1.394000</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>7.250</td>\n",
       "      <td>s volcano hawaii</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>139</td>\n",
       "      <td>3.309186</td>\n",
       "      <td>2.496814</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>6.263</td>\n",
       "      <td>police investigating after ebike collided with...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>16</td>\n",
       "      <td>96</td>\n",
       "      <td>1.697714</td>\n",
       "      <td>2.108286</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>6.307</td>\n",
       "      <td>latest more homes razed by northern california...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword_0  keyword_1  keyword_2  keyword_3  keyword_4  keyword_5  \\\n",
       "0             0          0          0          0          0          0   \n",
       "1             0          0          0          0          0          0   \n",
       "2             0          0          0          0          0          0   \n",
       "3             0          0          0          0          0          0   \n",
       "4             0          0          0          0          0          0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "7608          0          0          0          0          0          0   \n",
       "7609          0          0          0          0          0          0   \n",
       "7610          0          0          0          0          0          0   \n",
       "7611          0          0          0          0          0          0   \n",
       "7612          0          0          0          0          0          0   \n",
       "\n",
       "      keyword_6  keyword_7  keyword_8  \\\n",
       "0             0          0          1   \n",
       "1             0          0          1   \n",
       "2             0          0          1   \n",
       "3             0          0          1   \n",
       "4             0          0          1   \n",
       "...         ...        ...        ...   \n",
       "7608          0          0          1   \n",
       "7609          0          0          1   \n",
       "7610          0          0          1   \n",
       "7611          0          0          1   \n",
       "7612          0          0          1   \n",
       "\n",
       "                                                   text  continent_0  \\\n",
       "0     Our Deeds are the Reason of this #earthquake M...            0   \n",
       "1                Forest fire near La Ronge Sask. Canada            0   \n",
       "2     All residents asked to 'shelter in place' are ...            0   \n",
       "3     13,000 people receive #wildfires evacuation or...            0   \n",
       "4     Just got sent this photo from Ruby #Alaska as ...            0   \n",
       "...                                                 ...          ...   \n",
       "7608  Two giant cranes holding a bridge collapse int...            0   \n",
       "7609  @aria_ahrary @TheTawniest The out of control w...            0   \n",
       "7610  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...            0   \n",
       "7611  Police investigating after an e-bike collided ...            0   \n",
       "7612  The Latest: More Homes Razed by Northern Calif...            0   \n",
       "\n",
       "      continent_1  continent_2  continent_3  continent_4  country_0  \\\n",
       "0               0            0            0            1          0   \n",
       "1               0            0            0            1          0   \n",
       "2               0            0            0            1          0   \n",
       "3               0            0            0            1          0   \n",
       "4               0            0            0            1          0   \n",
       "...           ...          ...          ...          ...        ...   \n",
       "7608            0            0            0            1          0   \n",
       "7609            0            0            0            1          0   \n",
       "7610            0            0            0            1          0   \n",
       "7611            0            0            0            1          0   \n",
       "7612            0            0            0            1          0   \n",
       "\n",
       "      country_1  country_2  country_3  country_4  country_5  country_6  \\\n",
       "0             0          0          0          0          0          0   \n",
       "1             0          0          0          0          0          0   \n",
       "2             0          0          0          0          0          0   \n",
       "3             0          0          0          0          0          0   \n",
       "4             0          0          0          0          0          0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "7608          0          0          0          0          0          0   \n",
       "7609          0          0          0          0          0          0   \n",
       "7610          0          0          0          0          0          0   \n",
       "7611          0          0          0          0          0          0   \n",
       "7612          0          0          0          0          0          0   \n",
       "\n",
       "      country_7  city_0  city_1  city_2  city_3  city_4  city_5  city_6  \\\n",
       "0             1       0       0       0       0       0       0       0   \n",
       "1             1       0       0       0       0       0       0       0   \n",
       "2             1       0       0       0       0       0       0       0   \n",
       "3             1       0       0       0       0       0       0       0   \n",
       "4             1       0       0       0       0       0       0       0   \n",
       "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "7608          1       0       0       0       0       0       0       0   \n",
       "7609          1       0       0       0       0       0       0       0   \n",
       "7610          1       0       0       0       0       0       0       0   \n",
       "7611          1       0       0       0       0       0       0       0   \n",
       "7612          1       0       0       0       0       0       0       0   \n",
       "\n",
       "      city_7  city_8  city_9  city_10  hasNum  hasLink  hasUser  hasHashtag  \\\n",
       "0          0       0       0        1       0        0        0           1   \n",
       "1          0       0       0        1       0        0        0           0   \n",
       "2          0       0       0        1       0        0        0           0   \n",
       "3          0       0       0        1       1        0        0           1   \n",
       "4          0       0       0        1       0        0        0           1   \n",
       "...      ...     ...     ...      ...     ...      ...      ...         ...   \n",
       "7608       0       0       0        1       1        1        0           0   \n",
       "7609       0       0       0        1       0        0        1           0   \n",
       "7610       0       0       0        1       1        1        0           0   \n",
       "7611       0       0       0        1       0        0        0           0   \n",
       "7612       0       0       0        1       1        1        0           0   \n",
       "\n",
       "      exclamacion  interrogacion  mayusculas  longitud  Puntaje_ser_1  \\\n",
       "0               0              0          10        71       1.512000   \n",
       "1               0              0           5        40       1.697714   \n",
       "2               0              0           2       135       1.612000   \n",
       "3               0              0           1        67       2.966851   \n",
       "4               0              0           3        90       2.766851   \n",
       "...           ...            ...         ...       ...            ...   \n",
       "7608            0              0           7        85       1.732000   \n",
       "7609            0              0           6       127       1.857714   \n",
       "7610            0              1          10        67       1.412000   \n",
       "7611            0              0           4       139       3.309186   \n",
       "7612            0              0          16        96       1.697714   \n",
       "\n",
       "      Puntaje_ser_0  word_count  unique_word_count  url_count  \\\n",
       "0          2.294000          13                 13          0   \n",
       "1          2.108286           7                  7          0   \n",
       "2          2.194000          22                 20          0   \n",
       "3          4.839149           9                  8          0   \n",
       "4          4.039149          17                 15          0   \n",
       "...             ...         ...                ...        ...   \n",
       "7608       2.074000          11                 11          1   \n",
       "7609       2.948286          20                 17          0   \n",
       "7610       1.394000           8                  8          1   \n",
       "7611       2.496814          19                 19          0   \n",
       "7612       2.108286          13                 13          1   \n",
       "\n",
       "      mean_word_length                                      text_filtrado  \\\n",
       "0                4.384  our deeds are reason earthquake may allah forg...   \n",
       "1                4.571              forest fire near la ronge sask canada   \n",
       "2                5.090  residents asked shelter in place are being not...   \n",
       "3                7.125  people receive wildfires evacuation orders in ...   \n",
       "4                4.500  just got sent photo from ruby alaska smoke fro...   \n",
       "...                ...                                                ...   \n",
       "7608             6.636  two giant cranes holding bridge collapse into ...   \n",
       "7609             5.300  ariaahrary thetawniest out control wild fires ...   \n",
       "7610             7.250                                   s volcano hawaii   \n",
       "7611             6.263  police investigating after ebike collided with...   \n",
       "7612             6.307  latest more homes razed by northern california...   \n",
       "\n",
       "      contains_keyword  Tiene_key_impor  Tiene_key_no_impor  key_endoder  \\\n",
       "0                    0                0                   0     0.688525   \n",
       "1                    0                0                   0     0.688525   \n",
       "2                    0                0                   0     0.688525   \n",
       "3                    0                0                   0     0.688525   \n",
       "4                    0                0                   0     0.688525   \n",
       "...                ...              ...                 ...          ...   \n",
       "7608                 0                0                   0     0.688525   \n",
       "7609                 0                0                   0     0.688525   \n",
       "7610                 0                0                   0     0.688525   \n",
       "7611                 0                0                   0     0.688525   \n",
       "7612                 0                0                   0     0.688525   \n",
       "\n",
       "      Label_key  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "7608          0  \n",
       "7609          0  \n",
       "7610          0  \n",
       "7611          0  \n",
       "7612          0  \n",
       "\n",
       "[7613 rows x 54 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword_0</th>\n",
       "      <th>keyword_1</th>\n",
       "      <th>keyword_2</th>\n",
       "      <th>keyword_3</th>\n",
       "      <th>keyword_4</th>\n",
       "      <th>keyword_5</th>\n",
       "      <th>keyword_6</th>\n",
       "      <th>keyword_7</th>\n",
       "      <th>keyword_8</th>\n",
       "      <th>text</th>\n",
       "      <th>continent_0</th>\n",
       "      <th>continent_1</th>\n",
       "      <th>continent_2</th>\n",
       "      <th>continent_3</th>\n",
       "      <th>continent_4</th>\n",
       "      <th>country_0</th>\n",
       "      <th>country_1</th>\n",
       "      <th>country_2</th>\n",
       "      <th>country_3</th>\n",
       "      <th>country_4</th>\n",
       "      <th>country_5</th>\n",
       "      <th>country_6</th>\n",
       "      <th>country_7</th>\n",
       "      <th>city_0</th>\n",
       "      <th>city_1</th>\n",
       "      <th>city_2</th>\n",
       "      <th>city_3</th>\n",
       "      <th>city_4</th>\n",
       "      <th>city_5</th>\n",
       "      <th>city_6</th>\n",
       "      <th>city_7</th>\n",
       "      <th>city_8</th>\n",
       "      <th>city_9</th>\n",
       "      <th>city_10</th>\n",
       "      <th>hasNum</th>\n",
       "      <th>hasLink</th>\n",
       "      <th>hasUser</th>\n",
       "      <th>hasHashtag</th>\n",
       "      <th>exclamacion</th>\n",
       "      <th>interrogacion</th>\n",
       "      <th>mayusculas</th>\n",
       "      <th>longitud</th>\n",
       "      <th>Puntaje_ser_1</th>\n",
       "      <th>Puntaje_ser_0</th>\n",
       "      <th>word_count</th>\n",
       "      <th>unique_word_count</th>\n",
       "      <th>url_count</th>\n",
       "      <th>mean_word_length</th>\n",
       "      <th>text_filtrado</th>\n",
       "      <th>contains_keyword</th>\n",
       "      <th>Tiene_key_impor</th>\n",
       "      <th>Tiene_key_no_impor</th>\n",
       "      <th>key_endoder</th>\n",
       "      <th>Label_key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>1.706118</td>\n",
       "      <td>2.099882</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4.833</td>\n",
       "      <td>just happened terrible car crash</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>66</td>\n",
       "      <td>1.512000</td>\n",
       "      <td>2.294000</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>6.222</td>\n",
       "      <td>heard about earthquake different cities stay s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>98</td>\n",
       "      <td>1.928484</td>\n",
       "      <td>2.877516</td>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>4.105</td>\n",
       "      <td>there forest fire spot pond geese are fleeing ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Apocalypse lighting. #Spokane #wildfires</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>42</td>\n",
       "      <td>2.766851</td>\n",
       "      <td>4.039149</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>9.250</td>\n",
       "      <td>apocalypse lighting spokane wildfires</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>1.439778</td>\n",
       "      <td>2.366222</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>4.750</td>\n",
       "      <td>typhoon soudelor kills in china and taiwan</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>57</td>\n",
       "      <td>1.412000</td>\n",
       "      <td>1.394000</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>6.000</td>\n",
       "      <td>earthquake safety los angeles     safety faste...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Storm in RI worse than last hurricane. My city...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>141</td>\n",
       "      <td>2.558703</td>\n",
       "      <td>3.247297</td>\n",
       "      <td>23</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>5.086</td>\n",
       "      <td>storm in ri worse than last hurricane my harde...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Green Line derailment in Chicago http://t.co/U...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>57</td>\n",
       "      <td>1.652741</td>\n",
       "      <td>2.153259</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8.333</td>\n",
       "      <td>green line derailment in chicago</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "      <td>67</td>\n",
       "      <td>1.412000</td>\n",
       "      <td>1.394000</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>8.428</td>\n",
       "      <td>meg issues hazardous weather outlook hwo</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>70</td>\n",
       "      <td>1.685504</td>\n",
       "      <td>2.120496</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>7.625</td>\n",
       "      <td>cityofcalgary activated its municipal emergenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.688525</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword_0  keyword_1  keyword_2  keyword_3  keyword_4  keyword_5  \\\n",
       "0             0          0          0          0          0          0   \n",
       "1             0          0          0          0          0          0   \n",
       "2             0          0          0          0          0          0   \n",
       "3             0          0          0          0          0          0   \n",
       "4             0          0          0          0          0          0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3258          0          0          0          0          0          0   \n",
       "3259          0          0          0          0          0          0   \n",
       "3260          0          0          0          0          0          0   \n",
       "3261          0          0          0          0          0          0   \n",
       "3262          0          0          0          0          0          0   \n",
       "\n",
       "      keyword_6  keyword_7  keyword_8  \\\n",
       "0             0          0          1   \n",
       "1             0          0          1   \n",
       "2             0          0          1   \n",
       "3             0          0          1   \n",
       "4             0          0          1   \n",
       "...         ...        ...        ...   \n",
       "3258          0          0          1   \n",
       "3259          0          0          1   \n",
       "3260          0          0          1   \n",
       "3261          0          0          1   \n",
       "3262          0          0          1   \n",
       "\n",
       "                                                   text  continent_0  \\\n",
       "0                    Just happened a terrible car crash            0   \n",
       "1     Heard about #earthquake is different cities, s...            0   \n",
       "2     there is a forest fire at spot pond, geese are...            0   \n",
       "3              Apocalypse lighting. #Spokane #wildfires            0   \n",
       "4         Typhoon Soudelor kills 28 in China and Taiwan            0   \n",
       "...                                                 ...          ...   \n",
       "3258  EARTHQUAKE SAFETY LOS ANGELES ÛÒ SAFETY FASTE...            0   \n",
       "3259  Storm in RI worse than last hurricane. My city...            0   \n",
       "3260  Green Line derailment in Chicago http://t.co/U...            0   \n",
       "3261  MEG issues Hazardous Weather Outlook (HWO) htt...            0   \n",
       "3262  #CityofCalgary has activated its Municipal Eme...            0   \n",
       "\n",
       "      continent_1  continent_2  continent_3  continent_4  country_0  \\\n",
       "0               0            0            0            1          0   \n",
       "1               0            0            0            1          0   \n",
       "2               0            0            0            1          0   \n",
       "3               0            0            0            1          0   \n",
       "4               0            0            0            1          0   \n",
       "...           ...          ...          ...          ...        ...   \n",
       "3258            0            0            0            1          0   \n",
       "3259            0            0            0            1          0   \n",
       "3260            0            0            0            1          0   \n",
       "3261            0            0            0            1          0   \n",
       "3262            0            0            0            1          0   \n",
       "\n",
       "      country_1  country_2  country_3  country_4  country_5  country_6  \\\n",
       "0             0          0          0          0          0          0   \n",
       "1             0          0          0          0          0          0   \n",
       "2             0          0          0          0          0          0   \n",
       "3             0          0          0          0          0          0   \n",
       "4             0          0          0          0          0          0   \n",
       "...         ...        ...        ...        ...        ...        ...   \n",
       "3258          0          0          0          0          0          0   \n",
       "3259          0          0          0          0          0          0   \n",
       "3260          0          0          0          0          0          0   \n",
       "3261          0          0          0          0          0          0   \n",
       "3262          0          0          0          0          0          0   \n",
       "\n",
       "      country_7  city_0  city_1  city_2  city_3  city_4  city_5  city_6  \\\n",
       "0             1       0       0       0       0       0       0       0   \n",
       "1             1       0       0       0       0       0       0       0   \n",
       "2             1       0       0       0       0       0       0       0   \n",
       "3             1       0       0       0       0       0       0       0   \n",
       "4             1       0       0       0       0       0       0       0   \n",
       "...         ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "3258          1       0       0       0       0       0       0       0   \n",
       "3259          1       0       0       0       0       0       0       0   \n",
       "3260          1       0       0       0       0       0       0       0   \n",
       "3261          1       0       0       0       0       0       0       0   \n",
       "3262          1       0       0       0       0       0       0       0   \n",
       "\n",
       "      city_7  city_8  city_9  city_10  hasNum  hasLink  hasUser  hasHashtag  \\\n",
       "0          0       0       0        1       0        0        0           0   \n",
       "1          0       0       0        1       0        0        0           1   \n",
       "2          0       0       0        1       0        0        0           0   \n",
       "3          0       0       0        1       0        0        0           1   \n",
       "4          0       0       0        1       1        0        0           0   \n",
       "...      ...     ...     ...      ...     ...      ...      ...         ...   \n",
       "3258       0       0       0        1       0        0        0           0   \n",
       "3259       0       0       0        1       1        0        0           0   \n",
       "3260       0       0       0        1       0        1        0           0   \n",
       "3261       0       0       0        1       1        1        0           0   \n",
       "3262       0       0       0        1       0        0        0           1   \n",
       "\n",
       "      exclamacion  interrogacion  mayusculas  longitud  Puntaje_ser_1  \\\n",
       "0               0              0           1        36       1.706118   \n",
       "1               0              0           1        66       1.512000   \n",
       "2               0              0           1        98       1.928484   \n",
       "3               0              0           2        42       2.766851   \n",
       "4               0              0           4        47       1.439778   \n",
       "...           ...            ...         ...       ...            ...   \n",
       "3258            0              0          45        57       1.412000   \n",
       "3259            0              0           7       141       2.558703   \n",
       "3260            0              0           9        57       1.652741   \n",
       "3261            0              0          15        67       1.412000   \n",
       "3262            0              0           5        70       1.685504   \n",
       "\n",
       "      Puntaje_ser_0  word_count  unique_word_count  url_count  \\\n",
       "0          2.099882           6                  6          0   \n",
       "1          2.294000           9                  9          0   \n",
       "2          2.877516          19                 19          0   \n",
       "3          4.039149           4                  4          0   \n",
       "4          2.366222           8                  8          0   \n",
       "...             ...         ...                ...        ...   \n",
       "3258       1.394000           8                  7          0   \n",
       "3259       3.247297          23                 22          0   \n",
       "3260       2.153259           6                  6          1   \n",
       "3261       1.394000           7                  7          1   \n",
       "3262       2.120496           8                  8          0   \n",
       "\n",
       "      mean_word_length                                      text_filtrado  \\\n",
       "0                4.833                   just happened terrible car crash   \n",
       "1                6.222  heard about earthquake different cities stay s...   \n",
       "2                4.105  there forest fire spot pond geese are fleeing ...   \n",
       "3                9.250              apocalypse lighting spokane wildfires   \n",
       "4                4.750         typhoon soudelor kills in china and taiwan   \n",
       "...                ...                                                ...   \n",
       "3258             6.000  earthquake safety los angeles     safety faste...   \n",
       "3259             5.086  storm in ri worse than last hurricane my harde...   \n",
       "3260             8.333                   green line derailment in chicago   \n",
       "3261             8.428           meg issues hazardous weather outlook hwo   \n",
       "3262             7.625  cityofcalgary activated its municipal emergenc...   \n",
       "\n",
       "      contains_keyword  Tiene_key_impor  Tiene_key_no_impor  key_endoder  \\\n",
       "0                    0                0                   0     0.688525   \n",
       "1                    0                0                   0     0.688525   \n",
       "2                    0                0                   0     0.688525   \n",
       "3                    0                0                   0     0.688525   \n",
       "4                    0                0                   0     0.688525   \n",
       "...                ...              ...                 ...          ...   \n",
       "3258                 0                0                   0     0.688525   \n",
       "3259                 0                0                   0     0.688525   \n",
       "3260                 0                0                   0     0.688525   \n",
       "3261                 0                0                   0     0.688525   \n",
       "3262                 0                0                   0     0.688525   \n",
       "\n",
       "      Label_key  \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "...         ...  \n",
       "3258          0  \n",
       "3259          0  \n",
       "3260          0  \n",
       "3261          0  \n",
       "3262          0  \n",
       "\n",
       "[3263 rows x 54 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.replace({True: 1, False: 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['keyword_0', 'keyword_1', 'keyword_2', 'keyword_3', 'keyword_4',\n",
       "       'keyword_5', 'keyword_6', 'keyword_7', 'keyword_8', 'text',\n",
       "       'continent_0', 'continent_1', 'continent_2', 'continent_3',\n",
       "       'continent_4', 'country_0', 'country_1', 'country_2', 'country_3',\n",
       "       'country_4', 'country_5', 'country_6', 'country_7', 'city_0', 'city_1',\n",
       "       'city_2', 'city_3', 'city_4', 'city_5', 'city_6', 'city_7', 'city_8',\n",
       "       'city_9', 'city_10', 'hasNum', 'hasLink', 'hasUser', 'hasHashtag',\n",
       "       'exclamacion', 'interrogacion', 'mayusculas', 'longitud',\n",
       "       'Puntaje_ser_1', 'Puntaje_ser_0', 'word_count', 'unique_word_count',\n",
       "       'url_count', 'mean_word_length', 'text_filtrado', 'contains_keyword',\n",
       "       'Tiene_key_impor', 'Tiene_key_no_impor', 'key_endoder', 'Label_key'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tweets.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_processing(tweet):\n",
    "    \n",
    "    #Generating the list of words in the tweet (hastags and other punctuations removed)\n",
    "    def form_sentence(tweet):\n",
    "        tweet_blob = TextBlob(tweet)\n",
    "        return ' '.join(tweet_blob.words)\n",
    "    new_tweet = form_sentence(tweet)\n",
    "    \n",
    "    #Removing stopwords and words with unusual symbols\n",
    "    def no_user_alpha(tweet):\n",
    "        tweet_list = [ele for ele in tweet.split() if ele != 'user']\n",
    "        clean_tokens = [t for t in tweet_list if re.match(r'[^\\W\\d]*$', t)]\n",
    "        clean_s = ' '.join(clean_tokens)\n",
    "        clean_mess = [word for word in clean_s.split() if word.lower() not in stopwords.words('english')]\n",
    "        return clean_mess\n",
    "    no_punc_tweet = no_user_alpha(new_tweet)\n",
    "    \n",
    "    #Normalizing the words in tweets \n",
    "    def normalization(tweet_list):\n",
    "        lem = WordNetLemmatizer()\n",
    "        normalized_tweet = []\n",
    "        for word in tweet_list:\n",
    "            normalized_text = lem.lemmatize(word,'v')\n",
    "            normalized_tweet.append(normalized_text)\n",
    "        return normalized_tweet\n",
    "    \n",
    "    \n",
    "    return normalization(no_punc_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LIMPIO un poco\n",
    "remove(train_tweets, test_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_tweets\n",
    "test = test_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "msg_train, msg_test, label_train, label_test = train_test_split(x, y, test_size=0.2,random_state =87)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3889    flattened early birds get de cups lag bout pun...\n",
       "3051    contruction upgrading ferries earthquake stand...\n",
       "6724    severe thunderstorm warning including russellv...\n",
       "2302    charminar demolish in falling state anyway tak...\n",
       "5859    cause you play like symphony play till your fi...\n",
       "                              ...                        \n",
       "5371    we just laughing and talking junk now everyone...\n",
       "143          horrible accident man died in wings airplane\n",
       "2747    in fragile global economy considering devastat...\n",
       "333     sadly how windows reveals microsofts ethics ar...\n",
       "3558    financialtimes ethiopian regimes continue rece...\n",
       "Name: text, Length: 6090, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg_train[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOLO TEXT/TWEET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MULTINOMIAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8893267651888341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Pipeline.get_params of Pipeline(memory=None,\n",
       "         steps=[('bow2',\n",
       "                 CountVectorizer(analyzer=<function text_processing at 0x000001CBF9E9F3A8>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(2, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.3, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Machine Learning Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('bow',CountVectorizer(analyzer=text_processing,  ngram_range=(1, 1))),  # strings to token integer counts\n",
    "    ('bow2',CountVectorizer(analyzer=text_processing,  ngram_range=(2, 3))),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer(norm='l2')),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', MultinomialNB(alpha=1.3)),\n",
    "])\n",
    "pipeline.fit(msg_train[\"text\"],label_train)\n",
    "pred_train = pipeline.predict(msg_train[\"text\"])\n",
    "print(accuracy_score(pred_train,label_train))\n",
    "pipeline.get_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.76      0.83      1018\n",
      "           1       0.64      0.86      0.73       505\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.81      0.78      1523\n",
      "weighted avg       0.82      0.79      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[773 245]\n",
      " [ 71 434]]\n",
      "0.7925147734734077\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(msg_test[\"text\"])\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"text\"])\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7906834201654919\n",
      "F1 Score default:  0.7192766132346898\n"
     ]
    }
   ],
   "source": [
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       0\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       0\n",
       "3259  10865       0\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = submission_output(ids, predictions,\"Pipeline(tfidf y countvect (2,3) solo texto).csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hal9000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow2',\n",
       "                 CountVectorizer(analyzer=<function text_processing at 0x000001CBE88FE798>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(2, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\...\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='warn', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, Perceptron, LogisticRegression\n",
    "\n",
    "#Machine Learning Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('bow',CountVectorizer(analyzer=text_processing,  ngram_range=(1, 1))),  # strings to token integer counts\n",
    "    ('bow2',CountVectorizer(analyzer=text_processing,  ngram_range=(2, 3))),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer(norm='l2')),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', LogisticRegression()),\n",
    "])\n",
    "pipeline.fit(msg_train[\"text\"],label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83       974\n",
      "           1       0.68      0.84      0.75       549\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.78      0.81      0.79      1523\n",
      "weighted avg       0.82      0.80      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[754 220]\n",
      " [ 90 459]]\n",
      "0.7964543663821405\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(msg_test[\"text\"])\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7912963530493411\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"text\"])\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = submission_output(ids, predictions,\"Pipeline con lr(tfidf y countvect (2,3) solo texto).csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-5.40473453e-07,  9.71025119e-01,  6.73308668e-01, ...,\n",
       "        1.00000002e+00, -2.39754773e-01,  6.21855317e-01])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid search bayesiano"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow2',\n",
       "                 CountVectorizer(analyzer=<function text_processing at 0x0000018B8506DC18>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(2, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\...\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 LogisticRegressionCV(Cs=10, class_weight=None, cv=5,\n",
       "                                      dual=False, fit_intercept=True,\n",
       "                                      intercept_scaling=1.0, l1_ratios=None,\n",
       "                                      max_iter=1000, multi_class='ovr',\n",
       "                                      n_jobs=None, penalty='l2',\n",
       "                                      random_state=None, refit=True,\n",
       "                                      scoring=None, solver='liblinear',\n",
       "                                      tol=0.0001, verbose=0))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Machine Learning Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('bow',CountVectorizer(analyzer=text_processing,  ngram_range=(1, 1))),  # strings to token integer counts\n",
    "    ('bow2',CountVectorizer(analyzer=text_processing,  ngram_range=(2, 3))),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer(norm='l2')),  # integer counts to weighted TF-IDF scores\n",
    "])\n",
    "pipeline.fit(msg_train[\"text\"],label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.79      0.82       927\n",
      "           1       0.71      0.81      0.75       596\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.80      0.79      1523\n",
      "weighted avg       0.80      0.79      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[728 199]\n",
      " [116 480]]\n",
      "0.7931713722915299\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(msg_test[\"text\"])\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7916028194912658\n",
      "F1 Score default:  0.7362296353762607\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"text\"])\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = submission_output(ids, predictions,\"Pipeline con lrCV(tfidf y countvect (2,3) solo texto).csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow2',\n",
       "                 CountVectorizer(analyzer=<function text_processing at 0x000001CBE88FE798>,\n",
       "                                 binary=False, decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(2, 3), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\...\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 Perceptron(alpha=0.0001, class_weight=None,\n",
       "                            early_stopping=False, eta0=1.0, fit_intercept=True,\n",
       "                            max_iter=1000, n_iter_no_change=5, n_jobs=None,\n",
       "                            penalty=None, random_state=0, shuffle=True,\n",
       "                            tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "                            warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV, Perceptron, LogisticRegression\n",
    "\n",
    "#Machine Learning Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('bow',CountVectorizer(analyzer=text_processing,  ngram_range=(1, 1))),  # strings to token integer counts\n",
    "    ('bow2',CountVectorizer(analyzer=text_processing,  ngram_range=(2, 3))),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer(norm='l2')),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', Perceptron()),\n",
    "])\n",
    "pipeline.fit(msg_train[\"text\"],label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.76      0.78       892\n",
      "           1       0.69      0.74      0.71       631\n",
      "\n",
      "    accuracy                           0.75      1523\n",
      "   macro avg       0.75      0.75      0.75      1523\n",
      "weighted avg       0.76      0.75      0.76      1523\n",
      "\n",
      "\n",
      "\n",
      "[[681 211]\n",
      " [163 468]]\n",
      "0.7544320420223244\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(msg_test[\"text\"])\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7554397793441617\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"text\"])\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = submission_output(ids, predictions,\"Pipeline con Perceptron(tfidf y countvect (2,3) solo texto).csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8896551724137931\n"
     ]
    }
   ],
   "source": [
    "#Machine Learning Pipeline\n",
    "pipeline = Pipeline([\n",
    "    #('bow',CountVectorizer(analyzer=text_processing,  ngram_range=(1, 1))),  # strings to token integer counts\n",
    "    ('bow2',CountVectorizer(analyzer=text_processing,  ngram_range=(2, 3))),  # strings to token integer counts\n",
    "    ('tfidf', TfidfTransformer(norm='l2')),  # integer counts to weighted TF-IDF scores\n",
    "    ('classifier', XGBClassifier(n_estimators=1600,n_jobs=6,objective=\"binary:logistic\",\n",
    "                     max_depth= 60, learning_rate=0.01,subsample= 0.42,colsample_bytree=0.0933, gamma=1.0)),\n",
    "])\n",
    "pipeline.fit(msg_train[\"text\"],label_train)\n",
    "pred_train = pipeline.predict(msg_train[\"text\"])\n",
    "print(accuracy_score(pred_train,label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.77      0.83       976\n",
      "           1       0.67      0.83      0.74       547\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.80      0.78      1523\n",
      "weighted avg       0.81      0.79      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[752 224]\n",
      " [ 92 455]]\n",
      "0.7925147734734077\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(msg_test[\"text\"])\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7839411584431504\n"
     ]
    }
   ],
   "source": [
    "predictions = pipeline.predict(test[\"text\"])\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = submission_output(ids, predictions,\"Pipeline con XGBoost(tfidf y countvect (2,3) solo texto).csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO EL DF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_trans = ColumnTransformer(\n",
    "    [('city_category', OneHotEncoder(dtype='int'),['keyword']),\n",
    "    ('title_bow', CountVectorizer(analyzer=text_processing,  ngram_range=(2, 3)), 'text')],\n",
    "    remainder='drop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pipe y columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['keyword_0',\n",
       " 'keyword_1',\n",
       " 'keyword_2',\n",
       " 'keyword_3',\n",
       " 'keyword_4',\n",
       " 'keyword_5',\n",
       " 'keyword_6',\n",
       " 'keyword_7',\n",
       " 'keyword_8',\n",
       " 'continent_0',\n",
       " 'continent_1',\n",
       " 'continent_2',\n",
       " 'continent_3',\n",
       " 'continent_4',\n",
       " 'country_0',\n",
       " 'country_1',\n",
       " 'country_2',\n",
       " 'country_3',\n",
       " 'country_4',\n",
       " 'country_5',\n",
       " 'country_6',\n",
       " 'country_7',\n",
       " 'city_0',\n",
       " 'city_1',\n",
       " 'city_2',\n",
       " 'city_3',\n",
       " 'city_4',\n",
       " 'city_5',\n",
       " 'city_6',\n",
       " 'city_7',\n",
       " 'city_8',\n",
       " 'city_9',\n",
       " 'city_10',\n",
       " 'hasNum',\n",
       " 'hasLink',\n",
       " 'hasUser',\n",
       " 'hasHashtag',\n",
       " 'exclamacion',\n",
       " 'interrogacion',\n",
       " 'mayusculas',\n",
       " 'longitud',\n",
       " 'Puntaje_ser_1',\n",
       " 'Puntaje_ser_0',\n",
       " 'word_count',\n",
       " 'unique_word_count',\n",
       " 'url_count',\n",
       " 'mean_word_length',\n",
       " 'contains_keyword',\n",
       " 'Tiene_key_impor',\n",
       " 'Tiene_key_no_impor',\n",
       " 'key_endoder',\n",
       " 'Label_key']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = list(train_tweets.columns)\n",
    "col.remove(\"text\")\n",
    "col.remove('text_filtrado')\n",
    "#col.remove(\"mean_word_length\")\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Dejar(x):\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Pipeline\n",
    "\n",
    "# define transformers\n",
    "bow = CountVectorizer(analyzer=\"word\", ngram_range=(1, 3))\n",
    "bow1 = CountVectorizer(analyzer=\"char\", ngram_range=(1, 3))\n",
    "tfidf = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False,)\n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "bn = Binarizer()\n",
    "\n",
    "\n",
    "\n",
    "# set up pipelines for each column group\n",
    "key_pipe = Pipeline([\n",
    "                        ('ohe', ohe),\n",
    "                        #(\"bn\",bn ),\n",
    "                    ])\n",
    "text_pipe = Pipeline([\n",
    "                    ('bow', bow),\n",
    "                    #('bow1', bow1), \n",
    "                    ('tfidf', tfidf),\n",
    "                    ])\n",
    "idem_pipe = Pipeline([\n",
    "                    ('idem', FunctionTransformer(lambda x: x)), \n",
    "                    ])\n",
    "\n",
    "# define column transformer and set n_jobs to use all cores\n",
    "col_transformer = ColumnTransformer(\n",
    "                    transformers=[\n",
    "                        ('text', text_pipe, \"text\"),\n",
    "#                        ('key', key_pipe, [\"keyword\"]),\n",
    "                        ('Num',idem_pipe, col),\n",
    "                    ],\n",
    "                    remainder='drop',\n",
    "                    n_jobs=-1\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "pipe = Pipeline([\n",
    "            (\"preprocessing\", col_transformer),\n",
    "            #('xgboost', XGBClassifier(n_estimators=900,n_jobs=6,objective=\"binary:logistic\",\n",
    "             #                         max_depth= 50, learning_rate=0.01,subsample= 0.45,colsample_bytree=0.1, gamma=1.1)),\n",
    "            #('classifier', MultinomialNB(alpha=1.1))\n",
    "            #('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42,max_iter=5, tol=None)),\n",
    "            #('classifier', Perceptron()),\n",
    "            ('classifier', LogisticRegressionCV(n_jobs=4,Cs =25, max_iter=50,cv=10, solver =\"liblinear\", multi_class = \"ovr\",verbose =10))\n",
    "            #('classifier', LogisticRegression()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  10 | elapsed:  2.8min remaining:  2.8min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:  2.9min remaining:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  3.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.9866995073891626\n"
     ]
    }
   ],
   "source": [
    "pipe.fit(msg_train,label_train)\n",
    "\n",
    "pred_train = pipe.predict(msg_train)\n",
    "print(accuracy_score(pred_train,label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83       900\n",
      "           1       0.74      0.80      0.77       623\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.80      0.80      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[721 179]\n",
      " [123 500]]\n",
      "0.8017071569271176\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7992644805393809\n",
      "F1 Score default:  0.7534813699661271\n"
     ]
    }
   ],
   "source": [
    "predictions = pipe.predict(test)\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3258</th>\n",
       "      <td>10861</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3259</th>\n",
       "      <td>10865</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3260</th>\n",
       "      <td>10868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3261</th>\n",
       "      <td>10874</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3262</th>\n",
       "      <td>10875</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3263 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  target\n",
       "0         0       1\n",
       "1         2       1\n",
       "2         3       1\n",
       "3         9       1\n",
       "4        11       1\n",
       "...     ...     ...\n",
       "3258  10861       1\n",
       "3259  10865       1\n",
       "3260  10868       1\n",
       "3261  10874       1\n",
       "3262  10875       1\n",
       "\n",
       "[3263 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = submission_output(ids, predictions,\"Pipeline con L-RCV(tfidf y countvect (1,3),n_jobs=4,Cs =25,cv=10, solver =liblinear, multi_class = ovr,verbose =10,max_iter=100 ).csv\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tunning LogisticRegressionCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    #\"classifier__solver\" : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    #\"classifier__verbose\": [1,2,10,20],\n",
    "    \"classifier__max_iter\": [50,100,150,200,250,300,350,400,450],\n",
    "    #\"classifier__Cs\": [15,20,25,30],\n",
    "    #\"classifier__class_weight\":[None,\"balanced\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed: 13.0min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 24.7min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed: 40.0min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed: 57.2min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed: 76.8min\n",
      "[Parallel(n_jobs=4)]: Done  43 out of  45 | elapsed: 100.6min remaining:  4.7min\n",
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed: 101.5min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  10 | elapsed:  1.6min remaining:  1.6min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:  1.7min remaining:   43.2s\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hal9000\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9860426929392446\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, parameters, cv=5, n_jobs=4,verbose=10)\n",
    "b_m = gs.fit(msg_train,label_train)\n",
    "\n",
    "pred_train = b_m.predict(msg_train)\n",
    "print(accuracy_score(pred_train,label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__max_iter': 50}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.80      0.83       899\n",
      "           1       0.74      0.80      0.77       624\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.80      0.80      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[721 178]\n",
      " [123 501]]\n",
      "0.8023637557452397\n"
     ]
    }
   ],
   "source": [
    "predictions = b_m.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7998774134232303\n",
      "F1 Score default:  0.7544189544941707\n"
     ]
    }
   ],
   "source": [
    "predictions = b_m.predict(test)\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#leve mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    #\"classifier__solver\" : ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "    #\"classifier__verbose\": [1,2,10,20],\n",
    "    \"classifier__max_iter\": [50,100,150,200,250,300,350,400,450],\n",
    "    #\"classifier__Cs\": [15,20,25,30],\n",
    "    #\"classifier__class_weight\":[None,\"balanced\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed: 11.5min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed: 17.2min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed: 37.0min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed: 52.4min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 66.3min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed: 87.4min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 109.4min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed: 134.3min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 163.4min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed: 201.1min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 238.3min\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed: 287.0min\n",
      "[Parallel(n_jobs=4)]: Done 160 out of 160 | elapsed: 351.9min finished\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 out of  10 | elapsed:  3.3min remaining:  3.3min\n",
      "[Parallel(n_jobs=4)]: Done   7 out of  10 | elapsed:  3.4min remaining:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done  10 out of  10 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]0.9661740558292282\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, parameters, cv=5, n_jobs=4,verbose=10)\n",
    "b_m = gs.fit(msg_train,label_train)\n",
    "\n",
    "pred_train = b_m.predict(msg_train)\n",
    "print(accuracy_score(pred_train,label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'classifier__Cs': 30,\n",
       " 'classifier__class_weight': None,\n",
       " 'classifier__max_iter': 500}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83       904\n",
      "           1       0.73      0.80      0.77       619\n",
      "\n",
      "    accuracy                           0.80      1523\n",
      "   macro avg       0.80      0.80      0.80      1523\n",
      "weighted avg       0.81      0.80      0.80      1523\n",
      "\n",
      "\n",
      "\n",
      "[[723 181]\n",
      " [121 498]]\n",
      "0.8017071569271176\n"
     ]
    }
   ],
   "source": [
    "predictions = b_m.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7980386147716826\n",
      "F1 Score default:  0.7517890772128061\n"
     ]
    }
   ],
   "source": [
    "predictions = b_m.predict(test)\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO  mejora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tunning XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gamma y subsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'xgboost__gamma':[i/10 for i in range(20)],\n",
    "    'xgboost__subsample':[0.5,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 200 candidates, totalling 1000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  2.2min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  4.5min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  6.1min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  8.5min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed: 11.1min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 17.1min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed: 20.2min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 23.7min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed: 28.4min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 31.2min\n",
      "[Parallel(n_jobs=4)]: Done 137 tasks      | elapsed: 36.6min\n",
      "[Parallel(n_jobs=4)]: Done 154 tasks      | elapsed: 41.3min\n",
      "[Parallel(n_jobs=4)]: Done 173 tasks      | elapsed: 45.1min\n",
      "[Parallel(n_jobs=4)]: Done 192 tasks      | elapsed: 50.9min\n",
      "[Parallel(n_jobs=4)]: Done 213 tasks      | elapsed: 56.0min\n",
      "[Parallel(n_jobs=4)]: Done 234 tasks      | elapsed: 60.9min\n",
      "[Parallel(n_jobs=4)]: Done 257 tasks      | elapsed: 67.5min\n",
      "[Parallel(n_jobs=4)]: Done 280 tasks      | elapsed: 72.9min\n",
      "[Parallel(n_jobs=4)]: Done 305 tasks      | elapsed: 80.0min\n",
      "[Parallel(n_jobs=4)]: Done 330 tasks      | elapsed: 85.9min\n",
      "[Parallel(n_jobs=4)]: Done 357 tasks      | elapsed: 92.9min\n",
      "[Parallel(n_jobs=4)]: Done 384 tasks      | elapsed: 99.6min\n",
      "[Parallel(n_jobs=4)]: Done 413 tasks      | elapsed: 107.2min\n",
      "[Parallel(n_jobs=4)]: Done 442 tasks      | elapsed: 114.3min\n",
      "[Parallel(n_jobs=4)]: Done 473 tasks      | elapsed: 121.9min\n",
      "[Parallel(n_jobs=4)]: Done 504 tasks      | elapsed: 130.7min\n",
      "[Parallel(n_jobs=4)]: Done 537 tasks      | elapsed: 139.2min\n",
      "[Parallel(n_jobs=4)]: Done 570 tasks      | elapsed: 147.2min\n",
      "[Parallel(n_jobs=4)]: Done 605 tasks      | elapsed: 156.8min\n",
      "[Parallel(n_jobs=4)]: Done 640 tasks      | elapsed: 165.3min\n",
      "[Parallel(n_jobs=4)]: Done 677 tasks      | elapsed: 174.6min\n",
      "[Parallel(n_jobs=4)]: Done 714 tasks      | elapsed: 183.7min\n",
      "[Parallel(n_jobs=4)]: Done 753 tasks      | elapsed: 194.5min\n",
      "[Parallel(n_jobs=4)]: Done 792 tasks      | elapsed: 204.5min\n",
      "[Parallel(n_jobs=4)]: Done 833 tasks      | elapsed: 214.5min\n",
      "[Parallel(n_jobs=4)]: Done 874 tasks      | elapsed: 225.5min\n",
      "[Parallel(n_jobs=4)]: Done 917 tasks      | elapsed: 237.3min\n",
      "[Parallel(n_jobs=4)]: Done 960 tasks      | elapsed: 248.4min\n",
      "[Parallel(n_jobs=4)]: Done 1000 out of 1000 | elapsed: 258.4min finished\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, parameters, cv=5, n_jobs=4,verbose=10)\n",
    "b_m = gs.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'xgboost__gamma': 1.1, 'xgboost__subsample': 0.45}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      0.77      0.82       969\n",
      "           1       0.67      0.82      0.74       554\n",
      "\n",
      "    accuracy                           0.79      1523\n",
      "   macro avg       0.78      0.80      0.78      1523\n",
      "weighted avg       0.81      0.79      0.79      1523\n",
      "\n",
      "\n",
      "\n",
      "[[746 223]\n",
      " [ 98 456]]\n",
      "0.7892317793827971\n"
     ]
    }
   ],
   "source": [
    "predictions = b_m.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:  0.7870058228623966\n",
      "F1 Score default:  0.7221111555377848\n"
     ]
    }
   ],
   "source": [
    "predictions = b_m.predict(test)\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'xgboost__n_estimators':[50*i for i in range(40)],\n",
    "    #'xgboost__subsample':[0.5,0.1,0.15,0.2,0.25,0.3,0.35,0.4,0.45,0.5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 40 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  3.9min\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:  5.8min\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  7.4min\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:  9.7min\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed: 15.6min\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed: 18.8min\n",
      "[Parallel(n_jobs=4)]: Done 105 tasks      | elapsed: 23.4min\n"
     ]
    }
   ],
   "source": [
    "gs = GridSearchCV(pipe, parameters, cv=5, n_jobs=4,verbose=10)\n",
    "b_m = gs.fit(msg_train,label_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_m.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = b_m.predict(msg_test)\n",
    "\n",
    "print(classification_report(predictions,label_test))\n",
    "print ('\\n')\n",
    "print(confusion_matrix(predictions,label_test))\n",
    "print(accuracy_score(predictions,label_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = b_m.predict(test)\n",
    "\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "comprobar(predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
