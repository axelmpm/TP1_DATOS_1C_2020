{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "import enchant  #pip install pyenchant\n",
    "import sys,os\n",
    "from os import path\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import ast\n",
    "import nltk #pip install nltk\n",
    "            #nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import geonamescache  #pip install geonamescache\n",
    "import itertools\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE FUNCIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToDic(d,key,value):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un diccionario, una key y un value sumable con otros values\n",
    "    DEVUELVE: Nada porque el diccionario cambia globalmente al cambiarse aca\n",
    "    \"\"\"\n",
    "    \n",
    "    if key in d:\n",
    "        d[key] += value\n",
    "    else:\n",
    "        d[key] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeListOfLists(l):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de listas\n",
    "    DEVUELVE: Una lista que contiene como elementos los elementos de cada lista de la lista de listas\n",
    "    \"\"\"\n",
    "\n",
    "    return list(itertools.chain.from_iterable(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapNameLengToGeographical(geographicals):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una iterable de cosas geograficas del geonamescache\n",
    "    DEVUELVE: Un diccionario que tiene como k longitud del nombre \n",
    "                y como una lista de nombres con esa longitud\n",
    "    \"\"\"\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    for n in range(MAX_LOCATION_LENGTH_IN_WORDS):\n",
    "        for geographical in geographicals:\n",
    "\n",
    "            geographicalName = geographicals[geographical]['name']\n",
    "            nameParts = geographicalName.split(' ')\n",
    "            if len(nameParts) == n:\n",
    "                addToDic(dic,n,[geographicalName])\n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAcronim(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un texto\n",
    "    DEVUELVE: Devuelve un string solo con las mayusculas\n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    acronim = ''\n",
    "    for word in words:\n",
    "        if word[0].isupper():\n",
    "            acronim += word[0]\n",
    "    return acronim "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIABLES GLOBALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 690,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GLOBALES\n",
    "\n",
    "englishDictionary = enchant.Dict(\"en_US\")\n",
    "stopWords = set(stopwords.words('english'))\n",
    "alphabet = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','単','o','p','q','r','s','t','u',\n",
    "           'v','x','y','z']\n",
    "\n",
    "#WORD TYPE FLAGS\n",
    "\n",
    "EMPTY = '__EMPTY'\n",
    "WEB_ADDRESS = '__WEB_ADDRESS'\n",
    "ACRONIM = '__ACRONIM'\n",
    "NUMERIC = '__NUMERIC'\n",
    "NON_ALFABETICAL = '__NON_ALFABETICAL'\n",
    "\n",
    "\n",
    "# GEOGRAPHIC\n",
    "\n",
    "gc = geonamescache.GeonamesCache()\n",
    "\n",
    "MAX_LOCATION_LENGTH_IN_WORDS = 8 # no hay ciudades, paises o continentes cuyo nombre supere las 7 palabras\n",
    "\n",
    "continents = gc.get_continents()\n",
    "countries = gc.get_countries()\n",
    "usStates = gc.get_us_states()\n",
    "cities = gc.get_cities()\n",
    "\n",
    "citiyOrUSStateNameToCountryMap = {}\n",
    "\n",
    "for key in usStates:\n",
    "    stateName = usStates[key]['name']\n",
    "    addToDic(citiyOrUSStateNameToCountryMap,stateName,['United States'])\n",
    "    \n",
    "for key in cities:\n",
    "    cityName = cities[key]['name']\n",
    "    addToDic(citiyOrUSStateNameToCountryMap,cityName,[countries[cities[key]['countrycode']]['name']])\n",
    "\n",
    "nameLenToCityNameMap = mapNameLengToGeographical(cities)\n",
    "nameLenToUSStateNameMap = mapNameLengToGeographical(usStates)\n",
    "nameLenToCountryNameMap = mapNameLengToGeographical(countries)\n",
    "nameLenToContinentNameMap = mapNameLengToGeographical(continents)\n",
    "\n",
    "nameLenToCityOrUSStateNameMap = {}\n",
    "\n",
    "for k in nameLenToCityNameMap:\n",
    "    \n",
    "    if k in nameLenToUSStateNameMap:\n",
    "        addToDic(nameLenToCityOrUSStateNameMap,k,nameLenToUSStateNameMap[k])\n",
    "\n",
    "\n",
    "citiesNames = mergeListOfLists(list(nameLenToCityNameMap.values()))\n",
    "usStatesNames = mergeListOfLists(list(nameLenToUSStateNameMap.values()))\n",
    "citiesAndUsStatesNames = citiesNames + usStatesNames\n",
    "countriesNames = mergeListOfLists(list(nameLenToCountryNameMap.values())) + ['England','Ireland','Scotland', 'Brasil']\n",
    "continentsNames = mergeListOfLists(list(nameLenToContinentNameMap.values()))\n",
    "\n",
    "countryNameContinentNameMap = {}\n",
    "\n",
    "for k in countries:\n",
    "    \n",
    "    addToDic(countryNameContinentNameMap,countries[k]['name'],continents[countries[k]['continentcode']]['name'])\n",
    "\n",
    "addToDic(countryNameContinentNameMap,'England','Europe')\n",
    "addToDic(countryNameContinentNameMap,'Ireland','Europe')\n",
    "addToDic(countryNameContinentNameMap,'Scotland','Europe')\n",
    "addToDic(countryNameContinentNameMap,'Brasil','South America')\n",
    "    \n",
    "cityOrUSStateAcronimToNameMap = {}\n",
    "\n",
    "for acr in usStates:\n",
    "    addToDic(cityOrUSStateAcronimToNameMap,acr,[usStates[acr]['name']])\n",
    "\n",
    "for k in cities:\n",
    "    acr = getAcronim(cities[k]['name'])\n",
    "    addToDic(cityOrUSStateAcronimToNameMap,acr,[cities[k]['name']])\n",
    "    \n",
    "    \n",
    "cityOrStateToCountryMap = {}\n",
    "\n",
    "for acr in usStates:\n",
    "    addToDic(cityOrStateToCountryMap,usStates[acr]['name'],['United States'])\n",
    "\n",
    "for k in cities:\n",
    "    addToDic(cityOrStateToCountryMap,cities[k]['name'],[countries[cities[k]['countrycode']]['name']])\n",
    "    \n",
    "countryAcronimToCountryName = {}\n",
    "\n",
    "for acr in countries:\n",
    "    addToDic(countryAcronimToCountryName,acr,countries[acr]['name'])\n",
    "        \n",
    "UNKNOWN = '__UNKNOWN'\n",
    "CITY = '__CITY'\n",
    "COUNTRY = '__COUNTRY'\n",
    "CONTINENT = '__CONTINENT'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PROCESAMIENTO DE TEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordAfterSymbol(text,symbol):\n",
    "    \n",
    "    \"\"\" \n",
    "    Generalizando, como hizo mauro\n",
    "    \n",
    "    RECIBE: Un texto plano que puede tener cualquier cosa\n",
    "    DEVUELVE: Una lista de todos los strings que estaban precedidos por un simbolo(sea @, #, etc)\n",
    "    EJEMPLO: 'Hola que tal soy @axel, y vivo en @capitalFederal' -----> [axel,capitalFederal]'\n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    users = []\n",
    "    userToSave = \"\"\n",
    "    for word in words:\n",
    "        if len(word) != 0 and symbol in word:\n",
    "            splittedWord = word.split(symbol)\n",
    "            if len(splittedWord) > 1:\n",
    "                userToSave = splittedWord[1]\n",
    "            else:\n",
    "                userToSave = splittedWord[0]\n",
    "        if userToSave != '':\n",
    "            users.append(userToSave)\n",
    "            userToSave = \"\"\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasCertainWords(stuffWithWordsInIt,words):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Recibe algo que tiene palabras dentro (texto, lista de palabras, conjuntos con palabras, etc)\n",
    "    DEVUELVE: Devuelve True o False dependiendo de si el el contendor contiene ALGUNA palabra en la lista de palabras\n",
    "    \"\"\" \n",
    "    \n",
    "    for word in words:\n",
    "        if word in stuffWithWordsInIt:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acortar_texto(df):\n",
    "    \"\"\"\n",
    "    RECIBE: un df\n",
    "    DEVUELVE: el mismo df con una columna con version corta de la columna 'text'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df['short_text'] = 'None'\n",
    "    cant_filas = len(df.index)\n",
    "    for y in range(cant_filas):\n",
    "        df['short_text'][y:]= df.iloc[y][0][:15] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasQuestion(text):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: True si tiene al menos una palabra en ingles seguida por al menos un signo de pregunta, sino retorna FALSE\n",
    "    '''\n",
    "    text = text.replace('?', ' ? ')\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    text = takeOutSpecialCaractersFromText(text, specialCharacters = [',','.','#','!','%','*','(',')','\"','-','_', '$', ':', '/', '\\''])\n",
    "    textSplitted = text.split()\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            for j in range(textSplitted.index(word) + 1, len(textSplitted)):\n",
    "                if ('?' in textSplitted[j]):\n",
    "                    return True\n",
    "            return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeOutSpecialCaractersFromText(text, specialCharacters = None):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: El string sin caracteres especiales. Con el parametro specialCharacters se puede decidir que caracteres sacar.\n",
    "    '''\n",
    "    if specialCharacters == None:\n",
    "        specialCharacters = ['&','\\n','<','>','=',',','.','#','!','%','*','(',')','\"','-','_', '?', '$', ':', '/','\\''] + [str(i) for i in range(0,10)]\n",
    "    \n",
    "    \n",
    "    for char in specialCharacters:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateOutBiaSpecialCaractersAndMayuscFromText(text, specialCharacters = None):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: Una lista de los substrings que fuerons separados por los caracteres especiales o las mayusuclas.\n",
    "               Con el parametro specialCharacters se puede decidir que caracteres sacar.\n",
    "    '''\n",
    "    if specialCharacters == None:\n",
    "        specialCharacters = ['&','\\n','<','>','=',',','.','#','!','%','*','(',')',\n",
    "                             '\"',' ','-','_', '?', '$', ':', '/','\\''] + [str(i) for i in range(0,10)]\n",
    "        specialCharacters[specialCharacters.index(\"'\")] = '&' #HARDCODEADISIMO\n",
    "    \n",
    "    words = []\n",
    "    subWord = ''\n",
    "    prevC = ''\n",
    "    \n",
    "    for c in text:      \n",
    "        if c in specialCharacters:\n",
    "            if subWord != '':\n",
    "                words.append(subWord)\n",
    "                subWord = ''\n",
    "        else:\n",
    "            subWord += c\n",
    "        \n",
    "    if subWord != '':\n",
    "        words.append(subWord)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextFrom(data):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: El data frame de tweets\n",
    "    TENER EN CUENTA: \n",
    "            Considera el campo location, keyword y text\n",
    "            No altera al data frame\n",
    "    DEVUELVE: En una lista como elementos la concatenacion de campo keyword, location y text\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    data_copy['keyword'].fillna('', inplace = True)\n",
    "    data_copy['location'].fillna('', inplace = True)\n",
    "    \n",
    "    return list(data_copy['keyword'] + ' ' + data_copy['location'] + ' ' + data_copy['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isWebAddres(token):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un token\n",
    "    DEVUELVE: True o False dependiendo de si contiene el string http\n",
    "    \"\"\"  \n",
    "    \n",
    "    return 'http' in token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAcronim(token):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un token\n",
    "    DEVUELVE: True o False dependiendo de si el token tiene pinta de acronimo\n",
    "    \"\"\"  \n",
    "    \n",
    "    MAX_ACRONIM_LEN = 5\n",
    "    MIN_ACRONIM_LEN = 2\n",
    "    \n",
    "    if len(token) <= MAX_ACRONIM_LEN and len(token) >= MIN_ACRONIM_LEN:\n",
    "        \n",
    "        allUpper = True\n",
    "        for c in token:\n",
    "            if not c.isupper():\n",
    "                allUpper = False\n",
    "        \n",
    "        return allUpper\n",
    "    \n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allNumeric(token):\n",
    "    \"\"\" \n",
    "    RECIBE: Un token\n",
    "    DEVUELVE: True o False dependiendo de si todos los caracteres son numericos\n",
    "    EJEMPLO: 'hol4' -----> False\n",
    "    \"\"\"    \n",
    "    allNumeric = True\n",
    "    for c in token:\n",
    "        if not c.isnumeric():\n",
    "            allNumeric = False\n",
    "    return allNumeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumeric(token):\n",
    "    \"\"\" \n",
    "    RECIBE: Un token cualquiera\n",
    "    DEVUELVE: True o False dependiendo de si contiene algun numero\n",
    "    EJEMPLO: 'hol4' -----> True\n",
    "    \"\"\"    \n",
    "    \n",
    "    for c in token:\n",
    "        if c.isnumeric():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonAlfabetical(token):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un token\n",
    "    DEVUELVE: True o False dependiendo de si todo los caracteres son no alfabeticos\n",
    "    \"\"\"  \n",
    "    \n",
    "    allNotAlphabetical = True\n",
    "    for c in token:\n",
    "        if c in alphabet:\n",
    "            allNotAlphabetical = False\n",
    "        \n",
    "    return allNotAlphabetical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpToken(token):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un token\n",
    "    DEVUELVE: Una lista de palabras que surgen de limpiar el token o un flag que especifica que tipo de token es\n",
    "    \"\"\"  \n",
    "    \n",
    "    if token == '':\n",
    "        return [EMPTY]\n",
    "    \n",
    "    if isWebAddres(token):\n",
    "        return [WEB_ADDRESS]\n",
    "\n",
    "    if isAcronim(token):\n",
    "        return [ACRONIM]\n",
    "    \n",
    "    if allNumeric(token):\n",
    "        return [NUMERIC]\n",
    "    \n",
    "    if nonAlfabetical(token):\n",
    "        return [NON_ALFABETICAL]\n",
    "    \n",
    "    subTokens = separateOutBiaSpecialCaractersAndMayuscFromText(token.lower())\n",
    "\n",
    "    if len(subTokens) == 1:  \n",
    "        return [getEnlgishWord(subTokens[0]).lower()]\n",
    "    \n",
    "    else:\n",
    "        cleaned = []\n",
    "        for subToken in subTokens:\n",
    "            cleaned += cleanUpToken(subToken)\n",
    "\n",
    "        return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpTokens(textsTokens):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de listas de tokens\n",
    "    DEVUELVE: Limpia token a token para transformarlos en palabras validas segun cleanUpToken\n",
    "             y devuelve una lista (conservando repeticiones) de palabras validas\n",
    "             un map de palabras valida ----> [token1,token2,...,tokenk]\n",
    "             un map de token---->palabra valida\n",
    "    \"\"\"  \n",
    "\n",
    "    cleanWords = []\n",
    "    tokenCleanMap = {}\n",
    "    cleanTokenMap = {}\n",
    "    \n",
    "    for tokens in textsTokens:\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            words = cleanUpToken(token)\n",
    "            \n",
    "            if words == None:\n",
    "                pass\n",
    "                #print(token)\n",
    "            \n",
    "            for word in words:\n",
    "                \n",
    "                cleanWords.append(word)\n",
    "                addToDic(tokenCleanMap,token,word)\n",
    "                addToDic(cleanTokenMap,word,[token])\n",
    "            #print(\"{} -----> {}\".format(token,words))\n",
    "    \n",
    "    return cleanWords, tokenCleanMap, cleanTokenMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES DE INGLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnlgishWord(word):\n",
    "       \n",
    "    \"\"\"\n",
    "    RECIBE: Un texto plano que puede tener cualquier cosa\n",
    "    TENER EN CUENTA: Si la palabra es vacia retorna la palabra vacia\n",
    "                     No distingue mayusculas\n",
    "    DEVUELVE: La palabra en ingles mas parecida\n",
    "    \"\"\"  \n",
    "    failure = False\n",
    "    \n",
    "    if word == None:\n",
    "        failure = True\n",
    "        \n",
    "    if word == '':\n",
    "        failure = True\n",
    "        \n",
    "    else:\n",
    "        englishWord = getMaxSimilarityWord(word,englishDictionary.suggest(word))[0]\n",
    "    \n",
    "    if failure:  \n",
    "        return word\n",
    "    else:\n",
    "        return englishWord\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasEnglishWord(text):\n",
    "    \"\"\"\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: True si tiene al menos una palabra en ingles, sino retorna FALSE\n",
    "    \"\"\"\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    text = takeOutSpecialCaractersFromText(text)\n",
    "    textSplitted = text.split()\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishPercentage(text):\n",
    "    text = text.lower()\n",
    "    text = takeOutSpecialCaractersFromText(text)\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    textSplitted = text.split()\n",
    "    cantidadDePalabrasEnIngles = 0\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            cantidadDePalabrasEnIngles += 1\n",
    "    return (cantidadDePalabrasEnIngles / len(textSplitted)) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VARIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapBinaryLabel(binaryIndicator, labelsList):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: 1 o 0\n",
    "    DEVUELVE: labelsList[0] o labelsList[1] respectivamente\n",
    "    \"\"\"  \n",
    "    \n",
    "    if binaryIndicator == 1:\n",
    "        return labelsList[0]\n",
    "    if binaryIndicator == 0:\n",
    "        return labelsList[1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllConstantSum(k,n):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: k es el valor a sumar de forma constante, n es la cantidad de numeros a sumar\n",
    "    DEVUELVE: Todas las listas de n numeros que sumados dan k \n",
    "    \"\"\" \n",
    "    \n",
    "    if n == 1:\n",
    "        return [[k]]\n",
    "    \n",
    "    sums = []\n",
    "    \n",
    "    for i in range(k + 1):\n",
    "        \n",
    "        subsums = getAllConstantSum(k - i, n -1)\n",
    "        \n",
    "        for subsum in subsums:\n",
    "            sums.append([i] + subsum)\n",
    "    \n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isIn(element,container):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un elemento y un contenedor de elementos\n",
    "    DEVUELVE: Si el elemento esta o no en el contenedor\n",
    "    \"\"\" \n",
    "    \n",
    "    return element in container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createLengthDic(words):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un conjunto de palabras\n",
    "    DEVUELVE: Un diccionario que tiene como key longitudes y values listas de palabras con esas longitudes\n",
    "    EJEMPLO: words = ['a','b','aa','bbb'] ---> {1:['a','b'], 2:['aa'], 3:['bbb']}\n",
    "    \"\"\" \n",
    "    \n",
    "    d = {}\n",
    "    \n",
    "    for word in words:\n",
    "        n = len(word)\n",
    "        \n",
    "        addToDic(d,n,[word])\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSortedLengthValues(queryWord,words,lengths):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra sobre la que se va a comparar longitudes, un conjunto de palabras\n",
    "            y el conjunto de las longitudes que poseen esas palabras recibidas\n",
    "    DEVUELVE: Una lista ordenada de menor a mayor que tiene como elementos las diferencias\n",
    "                (no en modulo) de las longitudes entre cada palabra en words y la queryWord\n",
    "    EJEMPLO: query = 'aa', words = ['a','aa','aaa','aaaa'], lengths = [1,2,3,4] -----> [0,-1,1,2]\n",
    "    \"\"\" \n",
    "    \n",
    "    temp_lengthCandidates = {length - len(queryWord) : abs(length - len(queryWord)) for length in lengths}\n",
    "    lengthCandidates = [length - len(queryWord) for length in lengths]\n",
    "    return sorted(lengthCandidates, key = temp_lengthCandidates.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalizeText(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un texto\n",
    "    DEVUELVE: Ese texto pero con mayusculas al principio de cada palabra\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    capitalized = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        capitalized.append(token.capitalize())\n",
    "    return ' '.join(capitalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PERSISTENCIA (CARGA Y GUARDADO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAbsolutePath(fileName, exists = False, nombreRepo = 'DatosRepo'):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo\n",
    "            exists es para indicar que el archivo ya existe. En cuyo caso lo va a buscar a donde este\n",
    "    DEVUELVE: El path absoluto del archivo\n",
    "    TENER EN CUENTA: Si se pone mal escrito el archivo es posible que lo encuentre igual por ej 'trai' en vez de 'train.csv'\n",
    "                     Asume que tu repo se llama DatosRepo a menos que se lo digas en <nombreRepo>\n",
    "    EJEMPLO: 'miArchivo.txt' -----> '/home/axelmpm/DatosRepo/miArchivo.txt'\n",
    "    \"\"\"\n",
    "    \n",
    "    path = '/home/' + getpass.getuser() + '/' + nombreRepo\n",
    "    \n",
    "    if exists:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in [f for f in filenames if (os.path.basename(f) == fileName)]:\n",
    "                return os.path.join(dirpath, filename)\n",
    "    else:\n",
    "        return os.path.abspath(\"\") + '/' + fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDic(fileName, transformStringedListToList = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo .csv\n",
    "    DEVUELVE: El diccionario que estaba almacenado en disco\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(generateAbsolutePath(fileName + '.csv'))\n",
    "    \n",
    "    if transformStringedListToList:\n",
    "        df['values'] = df.apply(lambda row : ast.literal_eval(row['values']), axis = 1)\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    keys = list(df['keys'])\n",
    "    values = list(df['values'])\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        dic[keys[i]] = values[i]\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordInListFromFile(fileName):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo donde cada linea tiene palabras\n",
    "    DEVUELVE: Una lista donde cada elemento son las palabras que estaban como linea en el archivo (no toma las palabras de longitud 1)\n",
    "    \"\"\" \n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'r')\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for line in file:\n",
    "        if len(line[:-1]) > 1:\n",
    "            l.append(line[:-1]) #le saco el \\n\n",
    "    file.close()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFromFileIntoList(fileName, strings = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un nombre de archivo que se supone que almacena una lista previamente almacenada \n",
    "            con el metodo listToFile\n",
    "    DEVUELVE: La lista guardada en ese archivo\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'r')\n",
    "    \n",
    "    strList = (file.read()).split('&&&')[:-1]\n",
    "    \n",
    "    recoveredList = []\n",
    "    \n",
    "    for e in strList:\n",
    "        \n",
    "        if strings:\n",
    "            recoveredList.append(e)\n",
    "        else:\n",
    "            recoveredList.append(ast.literal_eval(e))\n",
    "        \n",
    "    return recoveredList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToFile(aList,fileName):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un nombre para el archivo que va a almacenar la lista y \n",
    "            una lista a almacenar\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'w')\n",
    "    \n",
    "    for e in aList:\n",
    "        file.write(str(e) + '&&&')\n",
    "        \n",
    "    file.close()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICCIONARIOS Y DATAFRAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrameOfFrecuencies(elementsLabel,elements, sort = True, sortAscending = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista o secuencia de elementos y un nombre para estos\n",
    "    DEVUELVE: Un dataframe con dos columnas, una de labels y otro de las frecuencias\n",
    "    \"\"\" \n",
    "    \n",
    "    temp_elements = list(elements.copy())\n",
    "    frecs_dict = {i:temp_elements.count(i) for i in temp_elements}\n",
    "\n",
    "    df = pd.DataFrame({elementsLabel : list(frecs_dict.keys()), 'Cantidad' : list(frecs_dict.values())})\n",
    "    \n",
    "    if sort:\n",
    "        df.sort_values('Cantidad', ascending = sortAscending, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicToDataFrame(dic, keysLabel = \"keys\", valuesLabel = \"values\", sort = True, sortAscending = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un diccionario y nombres para sus claves y valores y si si quiere ordenar o no ascendente o no\n",
    "    DEVUELVE: Un dataframe con dos columnas, una de keys y otro de las values\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({keysLabel : list(dic.keys()), valuesLabel : list(dic.values())})\n",
    "    \n",
    "    if sort:\n",
    "        df.sort_values(valuesLabel, ascending = sortAscending, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(x,y,data,xLabel = None,yLabel = None,title = '',\n",
    "            palette = 'Spectral',figX = 25,figY = 10,fontSize = 1.7):\n",
    "    \n",
    "    '''\n",
    "    RECIBE:\n",
    "        x: es el label de la columna con los datos de x del dataFrame 'data'\n",
    "        y: es el label de la columna con los datos de y del dataFrame 'data'\n",
    "        data: es el dataframe de donde sacar los datos\n",
    "        xLabel: es el label para x en el plot\n",
    "        yLabel: es el label para y en el plot\n",
    "        xLabel: es el titulo del plot\n",
    "        palette: es la paleta de colores a usar\n",
    "        figX: es el tama単o en x del plot\n",
    "        figY: es el tama単o en y del plot\n",
    "        fontSize: es el tama単o del font en el plot\n",
    "    A TENER EN CUENTA: No afecta al dataframe 'data'\n",
    "    DEVUELVE: El plot\n",
    "    '''\n",
    "    \n",
    "    if xLabel == None:\n",
    "        xLabel = x\n",
    "    \n",
    "    if yLabel == None:\n",
    "        yLabel = y\n",
    "    \n",
    "    sns.set(rc = {'figure.figsize':(figX,figY)})\n",
    "    sns.set(font_scale = fontSize)\n",
    "    v = sns.barplot(x = x, y = y, data = data, \n",
    "                 palette = palette, saturation = 1)\n",
    "    \n",
    "    v.set(xlabel = xLabel, ylabel= yLabel)\n",
    "    v.set(title = title)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordsFrequencies(targets,textsTokens,cleanWords,tokenCleanMap):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de tokens, unas lista de palabras traducidas de esos tokens (cleanWords)\n",
    "            y un diccionario que mapea token--->clean y una lista de targets que se corresponde\n",
    "            los textsTokens\n",
    "    DEVUELVE: Un diccionario de frecuencia de palabras en el dataframe y otro de frecuencia en desastres\n",
    "    \"\"\"\n",
    "    \n",
    "    disasterWordFrequency = {}\n",
    "    totalWordFrequency = {}\n",
    "    \n",
    "    i = 0\n",
    "    for tokens in textsTokens:\n",
    "        \n",
    "        for token in tokens:\n",
    "            \n",
    "            word = tokenCleanMap[token]\n",
    "            \n",
    "            addToDic(totalWordFrequency,word,text.count(word))\n",
    "            \n",
    "            if targets[i] == 1:\n",
    "                \n",
    "                addToDic(disasterWordFrequency,word,text.count(word))\n",
    "                \n",
    "        i += 1\n",
    "                    \n",
    "    return disasterWordFrequency,totalWordFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilityOfGivenThat(eventA,eventB,data):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un data frame y dos funciones booleanas que dado una row del data frame deciden corresponden\n",
    "            o no a los eventos o condiciones A y B\n",
    "    DEVUELVE: Devuelve la probabilidad condicional P(A|B) tomando como espacio muestral equiprobable el\n",
    "              dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    frequencyOfAandB = data[ data.apply(lambda row : eventA(row) and eventB(row), axis = 1)]['id'].count()\n",
    "    frequencyOfB = data[ data.apply(lambda row : eventB(row), axis = 1)]['id'].count()\n",
    "    \n",
    "    return frequencyOfAandB/frequencyOfB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES DE SIMILARIDAD EN TEXTOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHammingSimilarity(word1,word2):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos palabras de igual longitud\n",
    "    DEVUELVE: Un numero del 0 al 1 donde 1 significa que son el mismo string y 0 es que no se parecen en nada\n",
    "    EJEMPLO: word1 = casa, word2 = capa ---->  0.75 (porque tienen en comun la primera c, la segunda a\n",
    "             y la ultima a, osea 3/4 bien)\n",
    "    \"\"\"     \n",
    "    \n",
    "    length = len(word1)\n",
    "    mismatches = 0\n",
    "    \n",
    "    for i in range(length):\n",
    "        if word1[i] != word2[i]:\n",
    "            mismatches += 1\n",
    "    return 1 - mismatches/length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordExtentionsIntoNChars(word,n,extentionChar):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra a extender, la cantidad de caracteres que debe tener la version extendida y que caracter usar al extender\n",
    "    DEVUELVE: Devuelve un set de todas las posibles extensiones\n",
    "    EJEMPLO: word = 'casa', n = 6, extentionChar = '.' ---->\n",
    "                {'..casa','.c.asa','.ca.sa','.cas.a','.casa.','c..asa','c.a.sa','c.as.a','c.asa.','ca..sa'\n",
    "                    'ca.s.a','ca.sa.','cas..a','cas.a.','casa..'}   \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == '':\n",
    "        return [extentionChar * n]\n",
    "    \n",
    "    if (len(word) > n):\n",
    "        return [word]\n",
    "    \n",
    "    if len(word) == n:\n",
    "        return [word]\n",
    "    \n",
    "    wordLength = len(word)\n",
    "    amountOfCharsToAdd = n - wordLength\n",
    "    \n",
    "    extentions = set()\n",
    "    \n",
    "    amounts = getAllConstantSum(amountOfCharsToAdd,wordLength + 1)\n",
    "    \n",
    "    for amountOfExtendingChars in amounts:\n",
    "\n",
    "        extention = ''\n",
    "        for i in range(len(word)):\n",
    "            extention += (extentionChar * amountOfExtendingChars[i]) + word[i]\n",
    "        extention += extentionChar * amountOfExtendingChars[-1]\n",
    "        \n",
    "        extentions.add(''.join(list(extention)))\n",
    "    \n",
    "    return extentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(text1,text2):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos textos de cualquier longitud (no muy largos)\n",
    "    DEVUELVE: Un numero del 0 al 1 donde 1 significa que son el mismo string y 0 es que no se parecen en nada\n",
    "    A TENER EN CUENTA: Usa la similaridad de Hamming\n",
    "                       Es mas o menos costosa a partir de 20 caracteres de texto aprox\n",
    "                       Remueve los espacios por lo que no hace falta preprocesar texto\n",
    "    EJEMPLO: word1 = casa, word2 = ca ---->  0.5 (porque tienen en comun la primera c, la segunda a, osea 2/4 bien)\n",
    "    \"\"\" \n",
    "    \n",
    "    EMPTY_CHARACTER = ' '\n",
    "    \n",
    "    if text1 + text2 == '':\n",
    "        return 1\n",
    "    \n",
    "    if text1 + text2 == text1 or text1 + text2 == text2:\n",
    "        return 0\n",
    "    \n",
    "    splitedText1 = text1.split(' ')\n",
    "    splitedText2 = text2.split(' ')\n",
    "    \n",
    "    concatenatedWord1 = (''.join(splitedText1)).lower()\n",
    "    concatenatedWord2 = (''.join(splitedText2)).lower()\n",
    "    \n",
    "    if len(concatenatedWord1) > len(concatenatedWord2):\n",
    "        longest = concatenatedWord1\n",
    "        shortest = concatenatedWord2\n",
    "    else:\n",
    "        longest = concatenatedWord2\n",
    "        shortest = concatenatedWord1\n",
    "        \n",
    "    lenDif = len(longest) - len(shortest)\n",
    "    maxSimilarity = 0\n",
    "    extentions = generateWordExtentionsIntoNChars(shortest,len(longest),EMPTY_CHARACTER)\n",
    "    \n",
    "    for extention in extentions:\n",
    "        \n",
    "        subSimilarity = getHammingSimilarity(extention,longest)\n",
    "        \n",
    "        if (subSimilarity >= maxSimilarity):\n",
    "            maxSimilarity = subSimilarity\n",
    "            \n",
    "    return maxSimilarity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxSimilarityWord(queryWord, words, threshold = 1, lengthDic = None):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: El una palabra y palabras a comparar\n",
    "    TENER EN CUENTA: Si la palabras es vacia retorna la palabra vacia\n",
    "                    La lista de words debe ser chica porque esto es muy costoso\n",
    "                    busca hasta encontrar una palabra que supera el threshold\n",
    "    DEVUELVE: La palabra que mas se parece y su similitud\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(words) == 0:\n",
    "        return queryWord\n",
    "\n",
    "    if queryWord == '':\n",
    "        return queryWord\n",
    "    \n",
    "    maxSimilarity = 0\n",
    "    mostSimilarKnownWord = words[0]\n",
    "    \n",
    "    AMOUNT_COSIDERED_LITTLE = 100\n",
    "    \n",
    "    if len(words) < AMOUNT_COSIDERED_LITTLE:\n",
    "        THRESHOLD_SEARCH = len(words)\n",
    "    else:\n",
    "        THRESHOLD_SEARCH = AMOUNT_COSIDERED_LITTLE + len(words)/10\n",
    "    \n",
    "    if queryWord in words:\n",
    "        return queryWord\n",
    "    \n",
    "    if lengthDic == None:\n",
    "        lengthDic = createLengthDic(words)\n",
    "        \n",
    "    candidateLengthDiffs = makeSortedLengthValues(queryWord, words, lengthDic)\n",
    "    \n",
    "    found = False\n",
    "    searchLimitReached = False\n",
    "    searched = 0\n",
    "    \n",
    "    for lengthDiff in candidateLengthDiffs:\n",
    "        \n",
    "        if (len(queryWord) + lengthDiff) in lengthDic:\n",
    "            candidates = lengthDic[len(queryWord) + lengthDiff]\n",
    "        else:\n",
    "            continue\n",
    "        \n",
    "        for knownWord in candidates:\n",
    "\n",
    "            similarity = getSimilarity(queryWord,knownWord)\n",
    "\n",
    "            if maxSimilarity <= similarity:\n",
    "\n",
    "                maxSimilarity = similarity\n",
    "                mostSimilarKnownWord = knownWord\n",
    "\n",
    "            if similarity >= threshold:\n",
    "                found = True\n",
    "                break\n",
    "                \n",
    "            searched += 1\n",
    "            \n",
    "            if searched >= THRESHOLD_SEARCH:\n",
    "                searchLimitReached = True\n",
    "                break\n",
    "                \n",
    "        if found or searchLimitReached:\n",
    "            break\n",
    "    \n",
    "    return mostSimilarKnownWord, maxSimilarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSimilar(text1, text2, threshold = 0.7):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Dos textos (no muy grandes)\n",
    "    DEVUELVE: Si la similitud es mayor a un threshold\n",
    "    \"\"\" \n",
    "    \n",
    "    return getSimilarity(text1,text2) >= threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES DE LOCATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 689,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAKnownPlace(token):\n",
    "\n",
    "    \"\"\"\n",
    "    RECIBE: Un token cualquiera\n",
    "    A TENER EN CUENTA: Supone que esta bien escrito\n",
    "    DEVUELVE: Si el token es una ciudad o pais o continente y a que pertenece o UNKNOWN\n",
    "    \"\"\"\n",
    "    \n",
    "    capitalizedVersion = capitalizeText(token)\n",
    "    \n",
    "    if token in citiesAndUsStatesNames:\n",
    "        return True, CITY, token\n",
    "    else:\n",
    "        if capitalizedVersion in citiesAndUsStatesNames:\n",
    "            return True, CITY, capitalizedVersion\n",
    "    \n",
    "    if token in countriesNames:\n",
    "        return True, COUNTRY, token\n",
    "    else:\n",
    "        if capitalizedVersion in countriesNames:\n",
    "            return True, COUNTRY, capitalizedVersion\n",
    "    \n",
    "    if token in continentsNames:\n",
    "        return True, CONTINENT, token\n",
    "    \n",
    "    else:\n",
    "        if capitalizedVersion in continentsNames:\n",
    "            return True, CONTINENT, capitalizedVersion\n",
    "        \n",
    "    return False, UNKNOWN, UNKNOWN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCityOrStateByAcronimsAndCountry(acronims, country):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de acronimos a interpretar como ciudades y un pais que debe contener a esa ciudad\n",
    "    DEVUELVE: El nombre de la ciudad completo que esta contenido en el pais pasado o en caso contrario\n",
    "              UNKNOWN\n",
    "    \"\"\" \n",
    "    \n",
    "    for acr in acronims:\n",
    "        if acr in cityOrUSStateAcronimToNameMap:\n",
    "            asociatedCities = cityOrUSStateAcronimToNameMap[acr]\n",
    "            \n",
    "            for city in asociatedCities:\n",
    "                asociatedCountries = citiyOrUSStateNameToCountryMap[city]\n",
    "\n",
    "                for c in asociatedCountries:\n",
    "\n",
    "                    if country == c or country in c:\n",
    "                        return city\n",
    "                \n",
    "    return UNKNOWN    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 683,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountryByAcronimsAndContinent(acronims, continent):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de acronimos a interpretar como paises y un continent que debe contener a esa pais\n",
    "    DEVUELVE: El nombre de el pais completo que esta contenido en el continente pasado o en caso contrario\n",
    "              UNKNOWN\n",
    "    \"\"\" \n",
    "    \n",
    "    for acr in acronims:\n",
    "        if acr in countryAcronimToCountryName:\n",
    "            asociatedCountries = countryAcronimToCountryName[acr]\n",
    "\n",
    "            for country in asociatedCountries:\n",
    "\n",
    "                asociatedContinent = countryNameContinentNameMap[country]\n",
    "                if continent == c or continent in c:\n",
    "                    return country\n",
    "                \n",
    "    return UNKNOWN    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 684,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCountry(token, byCity = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un token y byCity que indica si el token es una ciudad de ese pais o no\n",
    "    DEVUELVE: La version del token en forma de pais (si existe) y si no devuelve UNKNOWN\n",
    "              En caso de byCity = True retorna el pais que contiene esa city \n",
    "              (ese token como city del pais)\n",
    "    \"\"\"\n",
    "    \n",
    "    if byCity:\n",
    "        return cityOrStateToCountryMap[token][0]\n",
    "    \n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 685,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContinent(token, byCountry = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un token y byCity que indica si el token es un pais de ese continente o no\n",
    "    DEVUELVE: La version del token en forma de continente (si existe) y si no devuelve UNKNOWN\n",
    "              En caso de byCity = True retorna el continente que contiene ese pais\n",
    "            (ese token como pais del continente)\n",
    "    \"\"\"\n",
    "    \n",
    "    if byCountry:\n",
    "        return countryNameContinentNameMap[token]\n",
    "    \n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizeLocation(text):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Texto crudo de una location\n",
    "    DEVUELVE: Una lista de 'palabras' (tokens) que por su 'significado' se supone que son entidades\n",
    "              separadas\n",
    "    \"\"\"\n",
    "    \n",
    "    return separateOutBiaSpecialCaractersAndMayuscFromText(str(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 687,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocation(rawLocation, threshold = 0.9):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Texto crudo de una location\n",
    "    TENER EN CUENTA: Tomamos como city a cualquier 'cosa' (ciudad, provincia, etc) \n",
    "                     que este dentro de un pais\n",
    "                     Asume que la gente escribe bien los lugares\n",
    "    DEVUELVE: Una tupla del tipo (continent,country,city)\n",
    "    \"\"\"\n",
    "    \n",
    "    if rawLocation == None or str(rawLocation) == 'nan' :\n",
    "        return (UNKNOWN,UNKNOWN,UNKNOWN)\n",
    "    \n",
    "    tokens = tokenizeLocation(rawLocation)\n",
    "    \n",
    "    city = UNKNOWN\n",
    "    country = UNKNOWN\n",
    "    continent = UNKNOWN\n",
    "    \n",
    "    cityFound = False\n",
    "    countryFound = False\n",
    "    continentFound = False\n",
    "    \n",
    "    if MAX_LOCATION_LENGTH_IN_WORDS > len(tokens):\n",
    "        maxWindowLength = len(tokens)\n",
    "    else:\n",
    "        maxWindowLength = MAX_LOCATION_LENGTH_IN_WORDS\n",
    "\n",
    "    acronims = []\n",
    "    \n",
    "    for window in range(maxWindowLength, 0, -1):\n",
    "        \n",
    "        identified = []\n",
    "        i = 0\n",
    "        while (i < (len(tokens) - window + 1) and (not cityFound or not countryFound or not continentFound)):\n",
    "            #print()\n",
    "            #print(\"WINDOW: {}\".format(window))\n",
    "            #print(i)\n",
    "            #print(\"COND1: {} | COND2: {}\".format(i < (len(tokens) - window + 1),(not cityFound or not countryFound or not continentFound)))\n",
    "            #print(\"TOKENS = {}\".format(tokens))\n",
    "            subTokens = tokens[i : i + window]\n",
    "            token = ' '.join(subTokens) \n",
    "            #print(\"TOKEN: {}\".format(token))\n",
    "            \n",
    "            isKnown, placeType, knownVersion = isAKnownPlace(token) #no mira acronimos\n",
    "            if isKnown:\n",
    "                identified += subTokens\n",
    "                \n",
    "                if placeType == CITY:\n",
    "                    cityFound = True\n",
    "                    city = knownVersion\n",
    "                    \n",
    "                elif placeType == COUNTRY:\n",
    "                    country = getCountry(knownVersion)\n",
    "                    continent = getContinent(country, byCountry = True)\n",
    "                    countryFound = True\n",
    "                    continentFound = True\n",
    "                    \n",
    "                elif placeType == CONTINENT:\n",
    "                    continent = getContinent(knownVersion)\n",
    "                    continentFound = True\n",
    "                    \n",
    "            if isAcronim(token):\n",
    "                acronims.append(token)\n",
    "                        \n",
    "            i += 1\n",
    "            \n",
    "        tokens = [t for t in tokens if not t in identified]\n",
    "            \n",
    "        if cityFound and countryFound and continentFound:\n",
    "            break;\n",
    "            \n",
    "    if cityFound and not countryFound:\n",
    "        country = getCountry(city, byCity = True)\n",
    "        continent = getContinent(country, byCountry = True)\n",
    "        countryFound = True\n",
    "        continentFound = True\n",
    "        \n",
    "    if not cityFound and countryFound and continentFound:\n",
    "        if len(acronims) > 0:\n",
    "            city = getCityOrStateByAcronimsAndCountry(acronims,country)\n",
    "            if city != UNKNOWN:\n",
    "                cityFound = True\n",
    "            \n",
    "    if not countryFound and continentFound:\n",
    "        if len(acronims) > 0:\n",
    "            country = getCountryByAcronimsAndContinent(acronims,continent)\n",
    "            countryFound = True\n",
    "            \n",
    "    if not cityFound and not countryFound and not continentFound:\n",
    "        if len(acronims) > 0:\n",
    "            for acr in acronims:\n",
    "                if acr in cityOrUSStateAcronimToNameMap:\n",
    "                    city = cityOrUSStateAcronimToNameMap[acr][0]\n",
    "                    cityFound = True\n",
    "                    break;            \n",
    "        \n",
    "        #print(\"TOKENS = {}\".format(tokens))\n",
    "    #print((continent,country, city))\n",
    "    return (continent,country, city)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES DE CONTEXTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addContextualKnowledge(tokens,contextualMap,tokenToCleanMap):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de tokens, un contexto de textos previos y una \n",
    "            'traduccion' del token a su version limpia (la escencia)\n",
    "    DEVUELVE: El contexto actualizado con el contenido del texto pasado\n",
    "    \"\"\"\n",
    "    \n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(len(tokens)):\n",
    "        for j in range(i + 1,len(tokens)):\n",
    "            if j < len(tokens):\n",
    "                pairs.append((tokenToCleanMap[tokens[i]],tokenToCleanMap[tokens[j]]))\n",
    "    \n",
    "    for pair in pairs:\n",
    "            \n",
    "        addToDic(contextualMap,pair[0],[pair[1]])\n",
    "        addToDic(contextualMap,pair[1],[pair[0]])\n",
    "            \n",
    "    return contextualMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualKnowledge(textsTokens,tokenToCleanMap):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de listas de tokens y una 'traduccion' del token a su version limpia (la escencia)\n",
    "    DEVUELVE: Un diccionario llamado contexto que tiene como claves combinaciones de palabras que se dieron\n",
    "                juntas en algun texto de la lista y cuya clave es la cantidad de textos en los que se dio\n",
    "    \"\"\"\n",
    "    \n",
    "    contextualMap = {}\n",
    "    \n",
    "    for tokens in textsTokens:\n",
    "        \n",
    "        contextualMap = addContextualKnowledge(tokens,contextualMap,tokenToCleanMap)\n",
    "    \n",
    "    return contextualMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReinforcement(contextualMap,word1,word2):\n",
    "    \n",
    "    if word1 in contextualMap:\n",
    "        return contextualMap[word1].count(word2)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUNCIONES DE NEGATIVIDAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordIntrinsicNegativity(word,disasterWordFrequency,totalWordFrequency):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra y los mapas de frecuencia en desastre y total\n",
    "    DEVUELVE: La frecuencia de uso de word en desastres / la frecuencia de word en todo el data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    if word in disasterWordFrequency and word in totalWordFrequency:\n",
    "        return disasterWordFrequency[word] / totalWordFrequency[word]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateNegativityMap(words,disasterWordFrequency,totalWordFrequency):\n",
    "   \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de palabras procesadas y los mapas de frecuencia en desastre y total\n",
    "    DEVUELVE: Un diccionario que tiene como key nouns y value su negatividad\n",
    "    \"\"\"\n",
    "    negativityMap = {}\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        negativityMap[word] = getWordIntrinsicNegativity(word,disasterWordFrequency,totalWordFrequency)\n",
    "        \n",
    "    return negativityMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualNegativity(queryWord,words,context):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra, una lista de palabras y un contexto\n",
    "    DEVUELVE: La negatividad contextual por la relacion entre estas palabras\n",
    "    \"\"\"\n",
    "    \n",
    "    contextualNegativity = 0\n",
    "    totalReinforcementMass = 0\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        reinforcement = getReinforcement(context.contextualMap,queryWord,word)\n",
    "        \n",
    "        asociates = set(context.contextualMap[queryWord])\n",
    "        \n",
    "        for asociate in asociates:\n",
    "            \n",
    "            totalReinforcementMass += getReinforcement(context.contextualMap,queryWord,asociate)\n",
    "        \n",
    "        contextualNegativity += reinforcement/totalReinforcementMass\n",
    "            \n",
    "    return contextualNegativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextNegativity(text,context):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un texto y un context\n",
    "    DEVUELVE: Un numero de 0 a 1 donde 1 es completamente negativo y 0 es nada negativo\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = text.split(' ')\n",
    "    \n",
    "    cleanedUpWords = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        \n",
    "        cleanedUpWords.append(context.tokenCleanMap[token])\n",
    "    \n",
    "    intrinsicNegativity = 0\n",
    "    contextualNegativity = 0\n",
    "    totalIntrinsicNegativity = 0\n",
    "    totalContextualNegativity = 0\n",
    "    contextualPonderation = 0\n",
    "    \n",
    "    for word in cleanedUpWords: \n",
    "        \n",
    "        intrinsicNegativity = context.negativityMap[word]\n",
    "        contextualNegativity =  getContextualNegativity(word,cleanedUpWords,context)\n",
    "        \n",
    "        if contextualNegativity > 0:\n",
    "            contextualPonderation += 1\n",
    "            \n",
    "        totalIntrinsicNegativity += intrinsicNegativity\n",
    "        totalContextualNegativity += contextualNegativity * intrinsicNegativity\n",
    "    \n",
    "    #print(\"TOTAL NEGATIVITY: {}, NEGATIVITY EXTRACTED FROM CONTEXT: {}\".format(totalIntrinsicNegativity + totalContextualNegativity * contextualPonderation,totalContextualNegativity * contextualPonderation))\n",
    "    return totalIntrinsicNegativity + totalContextualNegativity * contextualPonderation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASE CONTEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \n",
    "    \"\"\" \n",
    "    Clase que engloba toda las operaciones y nociones de contexto de un texto\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,data = None, similarityThreshold = 0.8, load = ''):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: \n",
    "            texts es una lista de textos (tweets) a entender\n",
    "            similarityThreshold es una tolerancia para la similaridad. \n",
    "                                Si dos palabras superan ese limite se consideran simialres\n",
    "            load es opcional y es el nombre del archivo que almacena\n",
    "                                el contextualMap guardado en disco en caso de haberlo hecho\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stopWords = set(stopwords.words('english'))\n",
    "        self.similarityThreshold = similarityThreshold\n",
    "        \n",
    "        self.__atrbt_texts = load + '_' + 'texts'\n",
    "        self.__atrbt_textsTokens = load + '_' + 'textsTokens'\n",
    "        self.__atrbt_cleanWords = load + '_' + 'cleanWords'\n",
    "        \n",
    "        self.__atrbt_contextualMap = load + '_' + 'contextualMap'\n",
    "        self.__atrbt_disasterCleanWordFrequency = load + '_' + 'disasterCleanWordFrequency'\n",
    "        self.__atrbt_totalCleanWordFrequency = load + '_' + 'totalCleanWordFrequency'\n",
    "        self.__atrbt_negativityMap = load + '_' + 'negativityMap'\n",
    "        \n",
    "        \n",
    "        #====================================================\n",
    "        self.data = data\n",
    "        print(\"Data loaded\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_texts):\n",
    "            self.texts = readFromFileIntoList(self.__atrbt_texts)\n",
    "            print(\"Texts loaded\")\n",
    "        else:\n",
    "            self.texts = extractTextFrom(data)\n",
    "            print(\"Extracted texts\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_textsTokens):\n",
    "            self.textsTokens = readFromFileIntoList(self.__atrbt_textsTokens, strings = True)\n",
    "            print(\"Texts Tokens loaded\")\n",
    "        else:\n",
    "            self.textsTokens = [tokenize(text) for text in self.texts]\n",
    "            print(\"Extracted Texts Tokens\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_cleanWords) and path.exists(self.__atrbt_tokenCleanMap) and path.exists(self.__atrbt_cleanTokenMap,):\n",
    "            \n",
    "            self.cleanWords = readFromFileIntoList(self.__atrbt_cleanWords)\n",
    "            print(\"Clean Words loaded\")\n",
    "            \n",
    "            self.tokenCleanMap = readDic(self.__atrbt_tokenCleanMap + '.csv')\n",
    "            print(\"Token Clean Words map loaded\")\n",
    "            \n",
    "            self.cleanTokenMap = readDic(self.__atrbt_cleanTokenMap + '.csv')\n",
    "            print(\"Clean Words Token map loaded\")\n",
    "            \n",
    "        else:\n",
    "            self.cleanWords, self.tokenCleanMap, self.cleanTokenMap = cleanUpTokens(self.textsTokens)\n",
    "            print(\"Tokens processed into cleaned up words and mapped\")\n",
    "            \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_contextualMap + '.csv'):\n",
    "            self.contextualMap = readDic(self.__atrbt_contextualMap, transformStringedListToList = True)\n",
    "            print(\"Contextual map loaded\")\n",
    "        else:\n",
    "            self.contextualMap = getContextualKnowledge(self.cleanWords)\n",
    "            print(\"Contextual map produced\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_disasterCleanWordFrequency + '.csv') and path.exists(self.__atrbt_totalCleanWordFrequency + '.csv'):\n",
    "            \n",
    "            self.disasterCleanWordFrequency = readDic(self.__atrbt_disasterCleanWordFrequency)\n",
    "            print(\"Disaster Frequency map loaded\")\n",
    "            \n",
    "            self.totalCleanWordFrequency = readDic(self.__atrbt_totalCleanWordFrequency)\n",
    "            print(\"Total Frequency map loaded\")\n",
    "        else:\n",
    "            self.disasterCleanWordFrequency, self.totalNounFrequency = extractWordsFrequencies(list(self.data['target']),self.textsTokens,self.cleanWords,self.tokenCleanMap)\n",
    "            print(\"Total and Disaster frequencies calculated for every word\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_negativityMap + '.csv'):\n",
    "            self.negativityMap = readDic(self.__atrbt_negativityMap)\n",
    "            print(\"Negativity map loaded\")\n",
    "        else:\n",
    "            self.negativityMap = calculateNegativityMap(self.cleanWords,self.disasterCleanWordFrequency,self.totalCleanWordFrequency)\n",
    "            print(\"Negativity map produced\")\n",
    "        \n",
    "        \n",
    "    def related(self, word, r = 1, restrict = True):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: La palabra de la que se quiere conocer sus asociados y el orden minimo de su asociacion (su reinforcement)\n",
    "                restric es para quedarse solo con las relaciones con palabras filtradas\n",
    "        DEVUELVE: Una lista de todos los asociados (palabras, orden) que cumplen con el r\n",
    "        \"\"\"\n",
    "        \n",
    "        if word in cleanWords:\n",
    "            asociated = self.contextualMap[word]\n",
    "        \n",
    "        elif word in tokenCleanMap:\n",
    "            asociated = self.contextualMap[tokenCleanMap[word]]\n",
    "        \n",
    "        else:\n",
    "            asociated = self.contextualMap[tokenCleanMap[getMaxSimilarityWord(word,list(self.tokenCleanMap.keys()))[0]]]\n",
    "            \n",
    "        if r == 1:\n",
    "            return asociated\n",
    "        \n",
    "        stronglyAsociated = []\n",
    "        \n",
    "        for pair in asociated:\n",
    "            if pair[1] >= r:\n",
    "                stronglyAsociated.append(pair)\n",
    "                                \n",
    "        return stronglyAsociated\n",
    "    \n",
    "    \n",
    "    def save(self,name):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: El nombre de archivo que guarda el contextualMap\n",
    "        DEVUELVE: Guarda el contextualMap en un csv\n",
    "        \"\"\"\n",
    "        \n",
    "        if name == '':\n",
    "            return None\n",
    "    \n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_texts):\n",
    "            \n",
    "            listToFile(self.texts, name + '_' + 'texts')\n",
    "            print(\"Texts saved\")\n",
    "            \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_textsTokens):\n",
    "            \n",
    "            listToFile(self.textsTokens, name + '_' + 'tokens') \n",
    "            print(\"Texts Tokens saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_cleanWords):\n",
    "            \n",
    "            listToFile(self.cleanWords, name + '_' + 'cleanWords') \n",
    "            print(\"Clean Words saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_tokenCleanMap + '.csv'):\n",
    "            \n",
    "            listToFile(self.tokenCleanMap, name + '_' + 'tokenCleanMap' + '.csv') \n",
    "            print(\"Token Clean Words map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_cleanTokenMap + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.cleanTokenMap, name + '_' + 'cleanTokenMap' + '.csv') \n",
    "            print(\"Clean Words Token map saved\")\n",
    "            \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_disasterCleanWordFrequency + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.disasterCleanWordFrequency).to_csv(name + '_' + 'disasterCleanWordFrequency' + '.csv')\n",
    "            print(\"Disaster Frequency map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_totalCleanWordFrequency + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.totalCleanWordFrequency).to_csv(name + '_' + 'totalCleanWordFrequency' + '.csv')\n",
    "            print(\"Total Frequency map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_negativityMap + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.negativityMap).to_csv(name + '_' + 'negativityMap' + '.csv')\n",
    "            print(\"Negativity map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_contextualMap + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.contextualMap).to_csv(name + '_' + 'contextualMap' + '.csv')\n",
    "            print(\"Contextual map saved\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
