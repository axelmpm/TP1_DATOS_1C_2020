{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "import enchant  #pip install pyenchant\n",
    "import sys,os\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import ast\n",
    "import nltk \n",
    "#pip install nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordAfterSymbol(text,symbol):\n",
    "    \n",
    "    \"\"\" \n",
    "    Generalizando, como hizo mauro\n",
    "    \n",
    "    RECIBE: Un texto plano que puede tener cualquier cosa\n",
    "    DEVUELVE: Una lista de todos los strings que estaban precedidos por un simbolo(sea @, #, etc)\n",
    "    EJEMPLO: 'Hola que tal soy @axel, y vivo en @capitalFederal' -----> [axel,capitalFederal]'\n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    users = []\n",
    "    userToSave = \"\"\n",
    "    for word in words:\n",
    "        if len(word) != 0 and symbol in word:\n",
    "            splittedWord = word.split(symbol)\n",
    "            if len(splittedWord) > 1:\n",
    "                userToSave = splittedWord[1]\n",
    "            else:\n",
    "                userToSave = splittedWord[0]\n",
    "        if userToSave != '':\n",
    "            users.append(userToSave)\n",
    "            userToSave = \"\"\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDots(word):\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra cualquiera\n",
    "    DEVUELVE: La parte de la palabra que no tiene ningun punto\n",
    "    EJEMPLO: 'esperando...' -----> 'esperando'\n",
    "    \"\"\"\n",
    "    \n",
    "    splited = word.split('.')\n",
    "    \n",
    "    for chunk in splited:\n",
    "        if chunk != '.':\n",
    "            return chunk\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumeric(word):\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra cualquiera\n",
    "    DEVUELVE: True o False dependiendo de si contiene algun numero\n",
    "    EJEMPLO: 'hol4' -----> True\n",
    "    \"\"\"    \n",
    "    \n",
    "    for c in word:\n",
    "        if c.isnumeric():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnlgishWord(word):\n",
    "       \n",
    "    \"\"\"\n",
    "    RECIBE: Un texto plano que puede tener cualquier cosa\n",
    "    DEVUELVE: La una tupla con la palabra mas larga en ingles que encuentre primero dentro del texto recibido\n",
    "              y un True o False dependiendo de si habia o no una palabra en ingles en el texto. \n",
    "              Si no contiene ninguna palabra en ingles devuelve el texto original.\n",
    "    TENER EN CUENTA: No considera como una palabra valida a las que tengan longitud 1\n",
    "    EJEMPLO: 'aaaaaaaaaaENGLISHaaaaaaaaaa' -----> 'ENGLISH'\n",
    "    EJEMPLO: 'aaaaaaaaaaaaaaaaaaaa' -----> 'aaaaaaaaaaaaaaaaaaaa'\n",
    "    \"\"\"  \n",
    "    \n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    MIN_LEN_WORDS = 2\n",
    "    \n",
    "    failure = False\n",
    "    \n",
    "    if word == None:\n",
    "        failure = True\n",
    "    \n",
    "    elif len(word) < MIN_LEN_WORDS:\n",
    "        failure = True\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(word), MIN_LEN_WORDS - 1,-1):\n",
    "            for j in range(i):\n",
    "                aux = word[j:i]\n",
    "                if englishDictionary.check(aux) and not hasNumeric(aux) and len(aux) > 1:\n",
    "                    return removeDots(aux), True\n",
    "                else:\n",
    "                    failure = True\n",
    "    \n",
    "    if failure:  \n",
    "        return word, False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapBinaryLabel(binaryIndicator, labelsList):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: 1 o 0\n",
    "    DEVUELVE: labelsList[0] o labelsList[1] respectivamente\n",
    "    \"\"\"  \n",
    "    \n",
    "    if binaryIndicator == 1:\n",
    "        return labelsList[0]\n",
    "    if binaryIndicator == 0:\n",
    "        return labelsList[1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAbsolutePath(fileName, exists = False, nombreRepo = 'DatosRepo'):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo\n",
    "            exists es para indicar que el archivo ya existe. En cuyo caso lo va a buscar a donde este\n",
    "    DEVUELVE: El path absoluto del archivo\n",
    "    TENER EN CUENTA: Si se pone mal escrito el archivo es posible que lo encuentre igual por ej 'trai' en vez de 'train.csv'\n",
    "                     Asume que tu repo se llama DatosRepo a menos que se lo digas en <nombreRepo>\n",
    "    EJEMPLO: 'miArchivo.txt' -----> '/home/axelmpm/DatosRepo/miArchivo.txt'\n",
    "    \"\"\"\n",
    "    \n",
    "    path = '/home/' + getpass.getuser() + '/' + nombreRepo\n",
    "    \n",
    "    if exists:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in [f for f in filenames if (os.path.basename(f) == fileName)]:\n",
    "                return os.path.join(dirpath, filename)\n",
    "    else:\n",
    "        return os.path.abspath(\"\") + '/' + fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDic(fileName, transformStringedListToList = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo .csv\n",
    "    DEVUELVE: El diccionario que estaba almacenado en disco\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(generateAbsolutePath(fileName + '.csv'))\n",
    "    \n",
    "    if transformStringedListToList:\n",
    "        df['values'] = df.apply(lambda row : ast.literal_eval(row['values']), axis = 1)\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    keys = list(df['keys'])\n",
    "    values = list(df['values'])\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        dic[keys[i]] = values[i]\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordInListFromFile(fileName):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo donde cada linea tiene palabras\n",
    "    DEVUELVE: Una lista donde cada elemento son las palabras que estaban como linea en el archivo (no toma las palabras de longitud 1)\n",
    "    \"\"\" \n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'r')\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for line in file:\n",
    "        if len(line[:-1]) > 1:\n",
    "            l.append(line[:-1]) #le saco el \\n\n",
    "    file.close()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasCertainWords(stuffWithWordsInIt,words):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Recibe algo que tiene palabras dentro (texto, lista de palabras, conjuntos con palabras, etc)\n",
    "    DEVUELVE: Devuelve True o False dependiendo de si el el contendor contiene ALGUNA palabra en la lista de palabras\n",
    "    \"\"\" \n",
    "    \n",
    "    for word in words:\n",
    "        if word in stuffWithWordsInIt:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasifyWordsRespectToEnglish(bagOfWords):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un contenedor de palabras que podrian o no estar en ingles\n",
    "    DEVUELVE: Una lista de palabras que estan en ingles y otra que no estaban en ingles\n",
    "    EJEMPLO: {'hello..gggggg,ghghghgh,who,hola'} -------> {hello,who}, {ghghghgh,hola}\n",
    "    \"\"\" \n",
    "    \n",
    "    bagOfEnglishWords = []\n",
    "    bagOfNonEnglishWords = []\n",
    "    for word in bagOfWords:\n",
    "        processedWord,isEnglish = getEnlgishWord(word)\n",
    "        if isEnglish:\n",
    "            bagOfEnglishWords.append(processedWord)\n",
    "        else:\n",
    "            bagOfNonEnglishWords.append(processedWord)\n",
    "        \n",
    "    return bagOfEnglishWords, bagOfNonEnglishWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrameOfFrecuencies(elementsLabel,elements, sort = True, sortAscending = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista o secuencia de elementos y un nombre para estos\n",
    "    DEVUELVE: Un dataframe con dos columnas, una de labels y otro de las frecuencias\n",
    "    \"\"\" \n",
    "    \n",
    "    temp_elements = list(elements.copy())\n",
    "    frecs_dict = {i:temp_elements.count(i) for i in temp_elements}\n",
    "\n",
    "    df = pd.DataFrame({elementsLabel : list(frecs_dict.keys()), 'Cantidad' : list(frecs_dict.values())})\n",
    "    \n",
    "    if sort:\n",
    "        df.sort_values('Cantidad', ascending = sortAscending, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicToDataFrame(dic, keysLabel = \"keys\", valuesLabel = \"values\", sort = True, sortAscending = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un diccionario y nombres para sus claves y valores y si si quiere ordenar o no ascendente o no\n",
    "    DEVUELVE: Un dataframe con dos columnas, una de keys y otro de las values\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({keysLabel : list(dic.keys()), valuesLabel : list(dic.values())})\n",
    "    \n",
    "    if sort:\n",
    "        df.sort_values(valuesLabel, ascending = sortAscending, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHammingSimilarity(word1,word2):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos palabras de igual longitud\n",
    "    DEVUELVE: Un numero del 0 al 1 donde 1 significa que son el mismo string y 0 es que no se parecen en nada\n",
    "    EJEMPLO: word1 = casa, word2 = capa ---->  0.75 (porque tienen en comun la primera c, la segunda a\n",
    "             y la ultima a, osea 3/4 bien)\n",
    "    \"\"\"     \n",
    "    \n",
    "    length = len(word1)\n",
    "    mismatches = 0\n",
    "    \n",
    "    for i in range(length):\n",
    "        if word1[i] != word2[i]:\n",
    "            mismatches += 1\n",
    "    return 1 - mismatches/length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllConstantSum(k,n):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: k es el valor a sumar de forma constante, n es la cantidad de numeros a sumar\n",
    "    DEVUELVE: Todas las listas de n numeros que sumados dan k \n",
    "    \"\"\" \n",
    "    \n",
    "    if n == 1:\n",
    "        return [[k]]\n",
    "    \n",
    "    sums = []\n",
    "    \n",
    "    for i in range(k + 1):\n",
    "        \n",
    "        subsums = getAllConstantSum(k - i, n -1)\n",
    "        \n",
    "        for subsum in subsums:\n",
    "            sums.append([i] + subsum)\n",
    "    \n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordExtentionsIntoNChars(word,n,extentionChar):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra a extender, la cantidad de caracteres que debe tener la version extendida y que caracter usar al extender\n",
    "    DEVUELVE: Devuelve un set de todas las posibles extensiones\n",
    "    EJEMPLO: word = 'casa', n = 6, extentionChar = '.' ---->\n",
    "                {'..casa','.c.asa','.ca.sa','.cas.a','.casa.','c..asa','c.a.sa','c.as.a','c.asa.','ca..sa'\n",
    "                    'ca.s.a','ca.sa.','cas..a','cas.a.','casa..'}   \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == '':\n",
    "        return [extentionChar * n]\n",
    "    \n",
    "    if (len(word) > n):\n",
    "        return [word]\n",
    "    \n",
    "    if len(word) == n:\n",
    "        return [word]\n",
    "    \n",
    "    wordLength = len(word)\n",
    "    amountOfCharsToAdd = n - wordLength\n",
    "    \n",
    "    extentions = set()\n",
    "    \n",
    "    amounts = getAllConstantSum(amountOfCharsToAdd,wordLength + 1)\n",
    "    \n",
    "    for amountOfExtendingChars in amounts:\n",
    "\n",
    "        extention = ''\n",
    "        for i in range(len(word)):\n",
    "            extention += (extentionChar * amountOfExtendingChars[i]) + word[i]\n",
    "        extention += extentionChar * amountOfExtendingChars[-1]\n",
    "        \n",
    "        extentions.add(''.join(list(extention)))\n",
    "    \n",
    "    return extentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(text1,text2):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos textos de cualquier longitud (no muy largos)\n",
    "    DEVUELVE: Un numero del 0 al 1 donde 1 significa que son el mismo string y 0 es que no se parecen en nada\n",
    "    A TENER EN CUENTA: Usa la similaridad de Hamming\n",
    "                       Es mas o menos costosa a partir de 20 caracteres de texto aprox\n",
    "                       Remueve los espacios por lo que no hace falta preprocesar texto\n",
    "    EJEMPLO: word1 = casa, word2 = ca ---->  0.5 (porque tienen en comun la primera c, la segunda a, osea 2/4 bien)\n",
    "    \"\"\" \n",
    "    \n",
    "    EMPTY_CHARACTER = ' '\n",
    "    \n",
    "    if text1 + text2 == '':\n",
    "        return 1\n",
    "    \n",
    "    if text1 + text2 == text1 or text1 + text2 == text2:\n",
    "        return 0\n",
    "    \n",
    "    splitedText1 = text1.split(' ')\n",
    "    splitedText2 = text2.split(' ')\n",
    "    \n",
    "    concatenatedWord1 = (''.join(splitedText1)).lower()\n",
    "    concatenatedWord2 = (''.join(splitedText2)).lower()\n",
    "    \n",
    "    if len(concatenatedWord1) > len(concatenatedWord2):\n",
    "        longest = concatenatedWord1\n",
    "        shortest = concatenatedWord2\n",
    "    else:\n",
    "        longest = concatenatedWord2\n",
    "        shortest = concatenatedWord1\n",
    "        \n",
    "    lenDif = len(longest) - len(shortest)\n",
    "    maxSimilarity = 0\n",
    "    extentions = generateWordExtentionsIntoNChars(shortest,len(longest),EMPTY_CHARACTER)\n",
    "    \n",
    "    for extention in extentions:\n",
    "        \n",
    "        subSimilarity = getHammingSimilarity(extention,longest)\n",
    "        #print(shortest,subWord,longest,subSimilarity)\n",
    "        \n",
    "        if (subSimilarity >= maxSimilarity):\n",
    "            maxSimilarity = subSimilarity\n",
    "            \n",
    "    return maxSimilarity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxSimilarityWord(word,words):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: El una palabra y palabras a comparar\n",
    "    DEVUELVE: La palabra que mas se parece\n",
    "    \"\"\"\n",
    "    \n",
    "    maxSimilarity = 0\n",
    "    mostSimilarKnownWord = ''\n",
    "    \n",
    "    if word in words:\n",
    "        return word\n",
    "    \n",
    "    candidateWords = []\n",
    "    for w in words:\n",
    "        if len(w) > 0 and w[0].lower() == word[0].lower():\n",
    "            candidateWords.append(w)\n",
    "    \n",
    "    for knownWord in candidateWords:\n",
    "        \n",
    "        similarity = getSimilarity(word,knownWord)\n",
    "\n",
    "        if maxSimilarity <= similarity:\n",
    "\n",
    "            maxSimilarity = similarity\n",
    "            mostSimilarKnownWord = knownWord\n",
    "            \n",
    "    return mostSimilarKnownWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSimilar(text1, text2, threshold = 0.7):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Dos textos (no muy grandes)\n",
    "    DEVUELVE: Si la similitud es mayor a un threshold\n",
    "    \"\"\" \n",
    "    \n",
    "    return getSimilarity(text1,text2) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isIn(element,container):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un elemento y un contenedor de elementos\n",
    "    DEVUELVE: Si el elemento esta o no en el contenedor\n",
    "    \"\"\" \n",
    "    \n",
    "    return element in container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acortar_texto(df):\n",
    "    \"\"\"\n",
    "    RECIBE: un df\n",
    "    DEVUELVE: el mismo df con una columna con version corta de la columna 'text'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df['short_text'] = 'None'\n",
    "    cant_filas = len(df.index)\n",
    "    for y in range(cant_filas):\n",
    "        df['short_text'][y:]= df.iloc[y][0][:15] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isQuestion(text):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: True si tiene al menos una palabra en ingles seguida por al menos un signo de pregunta, sino retorna FALSE\n",
    "    '''\n",
    "    text = text.replace('?', ' ? ')\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    text = takeOutSpecialCaractersFromText(text, specialCharacters = [',','.','#','!','%','*','(',')','\"','-','_', '$', ':', '/', '\\''])\n",
    "    textSplitted = text.split()\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            for j in range(textSplitted.index(word) + 1, len(textSplitted)):\n",
    "                if ('?' in textSplitted[j]):\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeOutSpecialCaractersFromText(text, specialCharacters = None):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: El string sin caracteres especiales. Con el parametro specialCharacters se puede decidir que caracteres sacar.\n",
    "    '''\n",
    "    if specialCharacters == None:\n",
    "        specialCharacters = ['&','\\n','<','>','=',',','.','#','!','%','*','(',')','\"','-','_', '?', '$', ':', '/','\\''] + [str(i) for i in range(0,10)]\n",
    "    \n",
    "    \n",
    "    for char in specialCharacters:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateOutBiaSpecialCaractersAndMayuscFromText(text, specialCharacters = None):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: Una lista de los substrings que fuerons separados por los caracteres especiales o las mayusuclas.\n",
    "               Con el parametro specialCharacters se puede decidir que caracteres sacar.\n",
    "    '''\n",
    "    if specialCharacters == None:\n",
    "        specialCharacters = ['&','\\n','<','>','=',',','.','#','!','%','*','(',')',\n",
    "                             '\"','-','_', '?', '$', ':', '/','\\''] + [str(i) for i in range(0,10)]\n",
    "    \n",
    "    words = []\n",
    "    subWord = ''\n",
    "    prevC = ''\n",
    "    \n",
    "    for c in text:\n",
    "        \n",
    "        if c.isupper() and prevC.islower():\n",
    "            words.append(subWord)\n",
    "            subWord = c\n",
    "        else:        \n",
    "            if c in specialCharacters:\n",
    "                if subWord != '':\n",
    "                    words.append(subWord)\n",
    "                    subWord = ''\n",
    "            else:\n",
    "                subWord += c\n",
    "        prevC = c\n",
    "        \n",
    "    if subWord != '':\n",
    "        words.append(subWord)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(x,y,data,xLabel = None,yLabel = None,title = '',\n",
    "            palette = 'Spectral',figX = 25,figY = 10,fontSize = 1.7):\n",
    "    \n",
    "    '''\n",
    "    RECIBE:\n",
    "        x: es el label de la columna con los datos de x del dataFrame 'data'\n",
    "        y: es el label de la columna con los datos de y del dataFrame 'data'\n",
    "        data: es el dataframe de donde sacar los datos\n",
    "        xLabel: es el label para x en el plot\n",
    "        yLabel: es el label para y en el plot\n",
    "        xLabel: es el titulo del plot\n",
    "        palette: es la paleta de colores a usar\n",
    "        figX: es el tamaño en x del plot\n",
    "        figY: es el tamaño en y del plot\n",
    "        fontSize: es el tamaño del font en el plot\n",
    "    A TENER EN CUENTA: No afecta al dataframe 'data'\n",
    "    DEVUELVE: El plot\n",
    "    '''\n",
    "    \n",
    "    if xLabel == None:\n",
    "        xLabel = x\n",
    "    \n",
    "    if yLabel == None:\n",
    "        yLabel = y\n",
    "    \n",
    "    sns.set(rc = {'figure.figsize':(figX,figY)})\n",
    "    sns.set(font_scale = fontSize)\n",
    "    v = sns.barplot(x = x, y = y, data = data, \n",
    "                 palette = palette, saturation = 1)\n",
    "    \n",
    "    v.set(xlabel = xLabel, ylabel= yLabel)\n",
    "    v.set(title = title)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addContextualKnowledge(wordList,context):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de palabras y un contexto de textos previos\n",
    "    DEVUELVE: El contexto actualizado con el contenido del texto pasado\n",
    "    \"\"\"\n",
    "    \n",
    "    words = list(set(wordList))\n",
    "    pairs = []\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        pairs.append((words[0],words[0]))\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i + 1,len(words)):\n",
    "                if j < len(words):\n",
    "                    pairs.append((words[i],words[j]))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        \n",
    "        if pair in context:\n",
    "            context[pair] += 1\n",
    "        else:\n",
    "            context[pair] = 1\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualKnowledge(texts):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de listas de palabras (lista de tweets)\n",
    "    DEVUELVE: Un diccionario llamado contexto que tiene como claves combinaciones de palabras que se dieron\n",
    "                juntas en algun texto de la lista y cuya clave es la cantidad de textos en los que se dio\n",
    "    \"\"\"\n",
    "    \n",
    "    context = {}\n",
    "    \n",
    "    for wordList in texts:\n",
    "        \n",
    "        context = addContextualKnowledge(wordList,context)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairsWith(word,context):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra y un contexto\n",
    "    DEVUELVE: Devuelve los pares de palabras del contexto que contienen a la palabra pasada\n",
    "    \"\"\"\n",
    "    \n",
    "    pairsWith = []\n",
    "    \n",
    "    for pair in context:\n",
    "        \n",
    "        if word in pair:\n",
    "            pairsWith.append(pair)\n",
    "    \n",
    "    return pairsWith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAsociatedWordsWithReinforcement(word,context):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra y un contexto\n",
    "    DEVUELVE: Una lista de (palabra,reinforcements) que seria como las palabras y la frecuencia con la que\n",
    "                las vio juntos a word y a palabra\n",
    "    \"\"\"\n",
    "    \n",
    "    asociatedPairs = getPairsWith(word,context)\n",
    "    asociatedWordAndReinforcement = []\n",
    "\n",
    "    for asociatedPair in asociatedPairs:\n",
    "        \n",
    "        if word != asociatedPair[0]:\n",
    "            asociatedWord = asociatedPair[0]\n",
    "        else:\n",
    "            asociatedWord = asociatedPair[1]\n",
    "\n",
    "        asociatedWordAndReinforcement.append((asociatedWord,context[asociatedPair]))\n",
    "  \n",
    "    return asociatedWordAndReinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualMap(context):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un contexto\n",
    "    DEVUELVE: Un diccionario que contiene como clave todas las palabras del contexto y como value sus asociadas\n",
    "                con su reinforcement\n",
    "    \"\"\"\n",
    "    \n",
    "    contextualMap = {}\n",
    "    \n",
    "    for pair in context:\n",
    "        \n",
    "        for word in pair:\n",
    "            \n",
    "            if not word in contextualMap:\n",
    "\n",
    "                asociatedWordAndReinforcement = getAsociatedWordsWithReinforcement(word,context)\n",
    "\n",
    "                contextualMap[word] = asociatedWordAndReinforcement\n",
    "            \n",
    "    return contextualMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeIntoWords(texts):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de lista de palabras\n",
    "    DEVUELVE: Una lista de las palabras de los textos recibidos\n",
    "    \"\"\"\n",
    "    return list(set(itertools.chain.from_iterable(texts)))                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpWords(texts):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de textos\n",
    "    DEVUELVE: Una lista de lista de palabras (lista de tweets) cuyas palabras de los textos recibidos no son vacias\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanedTexts = []\n",
    "    \n",
    "    stopWords =  set(stopwords.words('english'))\n",
    "    \n",
    "    for text in texts:\n",
    "    \n",
    "        words = text.split(' ')\n",
    "        cleanedWords = []\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            subWords = separateOutBiaSpecialCaractersAndMayuscFromText(word) \n",
    "            \n",
    "            for subWord in subWords:\n",
    "                \n",
    "                if subWord == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    cleanedWords.append(subWord.lower())\n",
    "        \n",
    "        cleanedTexts.append(cleanedWords)\n",
    "        \n",
    "    return cleanedTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def furtherCleanUp(words,stopWords):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de palabras y unas stopwords\n",
    "    DEVUELVE: Una lista de palabras que no tienen longitud menor a 3 ni son stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanWords = []\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if not (len(word) < 3 or word in stopWords):\n",
    "            cleanWords.append(word)\n",
    "            \n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncateMapTo(contextualMap,words):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Toma un contextual map y unas palabras\n",
    "    DEVUELVE: Un contextual map cuyas palabras en key y en los values no estan en la lista de palabras recibidas\n",
    "    \"\"\"\n",
    "    \n",
    "    truncatedContextualMap = {}\n",
    "    \n",
    "    for word in contextualMap:\n",
    "        \n",
    "        if word in words:\n",
    "            asociates = contextualMap[word]\n",
    "\n",
    "            filteredAsociates = []\n",
    "\n",
    "            i = 0\n",
    "            checksForEveryWordInAsociates = [pair[0] in words for pair in asociates]\n",
    "\n",
    "            for asociatedWordIsIn in checksForEveryWordInAsociates:\n",
    "\n",
    "                if asociatedWordIsIn:\n",
    "                    filteredAsociates.append(asociates[i])\n",
    "                i += 1\n",
    "\n",
    "            truncatedContextualMap[word] = filteredAsociates\n",
    "    return truncatedContextualMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFromFileIntoList(fileName, strings = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un nombre de archivo que se supone que almacena una lista previamente almacenada \n",
    "            con el metodo listToFile\n",
    "    DEVUELVE: La lista guardada en ese archivo\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'r')\n",
    "    \n",
    "    strList = (file.read()).split('&&&')[:-1]\n",
    "    \n",
    "    recoveredList = []\n",
    "    \n",
    "    for e in strList:\n",
    "        \n",
    "        if strings:\n",
    "            recoveredList.append(e)\n",
    "        else:\n",
    "            recoveredList.append(ast.literal_eval(e))\n",
    "        \n",
    "    return recoveredList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToFile(aList,fileName):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un nombre para el archivo que va a almacenar la lista y \n",
    "            una lista a almacenar\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'w')\n",
    "    \n",
    "    for e in aList:\n",
    "        file.write(str(e) + '&&&')\n",
    "        \n",
    "    file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \n",
    "    \"\"\" \n",
    "    Clase que engloba toda las operaciones y nociones de contexto de un texto\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,data = None, similarityThreshold = 0.8, load = None):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: \n",
    "            texts es una lista de textos (tweets) a entender\n",
    "            similarityThreshold es una tolerancia para la similaridad. \n",
    "                                Si dos palabras superan ese limite se consideran simialres\n",
    "            load es opcional y es el nombre del archivo que almacena\n",
    "                                el contextualMap guardado en disco en caso de haberlo hecho\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stopWords = set(stopwords.words('english'))\n",
    "        self.similarityThreshold = similarityThreshold\n",
    "        \n",
    "        if load == None:\n",
    "            \n",
    "            self.texts = cleanUpWords(texts)        \n",
    "            self.words = mergeIntoWords(self.texts)\n",
    "            self.wordNet = getContextualKnowledge(self.texts)\n",
    "            self.englishWords, self.nonEnglishWords = clasifyWordsRespectToEnglish(self.words)\n",
    "            self.nouns = furtherCleanUp(self.englishWords,self.stopWords)\n",
    "            self.contextualMap = getContextualMap(self.wordNet)\n",
    "            self.englishContextualMap = truncateMapTo(self.contextualMap,self.nouns)\n",
    "            \n",
    "        else:\n",
    "\n",
    "            __atrbt_texts = load + '_' + 'texts'\n",
    "            __atrbt_words = load + '_' + 'words'\n",
    "            __atrbt_wordNet = load + '_' + 'wordNet'\n",
    "            __atrbt_englishWords = load + '_' + 'englishWords'\n",
    "            __atrbt_nonEnglishWords = load + '_' + 'nonEnglishWords'\n",
    "            __atrbt_nouns = load + '_' + 'nouns'\n",
    "            __atrbt_contextualMap = load + '_' + 'contextualMap'\n",
    "            __atrbt_englishContextualMap = load + '_' + 'englishContextualMap'\n",
    "\n",
    "            self.texts = readFromFileIntoList(__atrbt_texts)        \n",
    "            self.words = readFromFileIntoList(__atrbt_words, strings = True)\n",
    "            self.wordNet = readDic(__atrbt_wordNet)\n",
    "            self.englishWords = readFromFileIntoList(__atrbt_englishWords, strings = True)\n",
    "            self.nonEnglishWords = readFromFileIntoList(__atrbt_nonEnglishWords, strings = True)\n",
    "            self.nouns = readFromFileIntoList(__atrbt_nouns, strings = True)\n",
    "            self.contextualMap = readDic(__atrbt_contextualMap, transformStringedListToList = True)\n",
    "            self.englishContextualMap = readDic(__atrbt_englishContextualMap, transformStringedListToList = True)           \n",
    "        \n",
    "        \n",
    "    def related(self, word, r = 1, restrict = True):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: La palabra de la que se quiere conocer sus asociados y el orden minimo de su asociacion (su reinforcement)\n",
    "                restric es para quedarse solo con las relaciones con palabras filtradas\n",
    "        DEVUELVE: Una lista de todos los asociados (palabras, orden) que cumplen con el r\n",
    "        \"\"\"\n",
    "        \n",
    "        if restrict:\n",
    "            asociated = self.englishContextualMap[getMaxSimilarityWord(word,self.words)]\n",
    "        else:\n",
    "            asociated = self.contextualMap[getMaxSimilarityWord(word,self.words)]\n",
    "        \n",
    "        if r == 1:\n",
    "            return asociated\n",
    "        \n",
    "        strongAsociated = []\n",
    "        \n",
    "        for pair in asociated:\n",
    "            if pair[1] >= r:\n",
    "                strongAsociated.append(pair)\n",
    "                                \n",
    "        return strongAsociated\n",
    "    \n",
    "    \n",
    "    def save(self,name):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: El nombre de archivo que guarda el contextualMap\n",
    "        DEVUELVE: Guarda el contextualMap en un csv\n",
    "        \"\"\"\n",
    "        \n",
    "        listToFile(self.texts, name + '_' + 'texts')\n",
    "        listToFile(self.words, name + '_' + 'words')       \n",
    "        listToFile(self.englishWords, name + '_' + 'englishWords')\n",
    "        listToFile(self.nonEnglishWords, name + '_' + 'nonEnglishWords')\n",
    "        listToFile(self.nouns, name + '_' + 'nouns')\n",
    "        \n",
    "        dicToDataFrame(self.wordNet).to_csv(name + '_' + 'wordNet' + '.csv')\n",
    "        dicToDataFrame(self.contextualMap).to_csv(name + '_' + 'contextualMap' + '.csv')\n",
    "        dicToDataFrame(self.englishContextualMap).to_csv(name + '_' + 'englishContextualMap' + '.csv')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNegativity(text,negativeWords):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un texto y un conjunto de palabras negativas\n",
    "    DEVUELVE: Un numero de 0 a 1 donde 1 es completamente negativo y 0 es nada negativo\n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    count = 0\n",
    "    for word in words:\n",
    "        for negativeWord in negativeWords:\n",
    "        \n",
    "            if isSimilar(word,negativeWord):\n",
    "                count += 1\n",
    "                break\n",
    "                \n",
    "    return count/len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estaria bueno ponderar las palabras negativas de alguna forma con un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Estoy implicando que mas negativo es un tweet cuando mas palabras negativas usa pero no por su peso\n",
    "\n",
    "#'robo robo robo robo robo' es mas negativo que 'en un terremoto en japon muerieron treinticinco millones de personas'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hay palabras que aumentan su peso por el contexto (otras palabras que estan) y hay palabras que ganan negatividad cuando hay otras\n",
    "\n",
    "#por ej personas no es negativo, pero si esta en la misma frase que muerieron, si lo es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asociar palabras con palabras a las que \"contagia\" de negatividad de un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "negs = ['romper','cortar','cuidado','robo','muerieron','terremoto']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'robo robo robo robo robo'1\n",
    "getNegativity(text,negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'en un terremoto en japon muieron treinticinco millones de personas'\n",
    "getNegativity(text,negs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
