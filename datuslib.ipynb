{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "\n",
    "import enchant  #pip install pyenchant\n",
    "import sys,os\n",
    "from os import path\n",
    "import getpass\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import ast\n",
    "import nltk \n",
    "#pip install nltk\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractWordAfterSymbol(text,symbol):\n",
    "    \n",
    "    \"\"\" \n",
    "    Generalizando, como hizo mauro\n",
    "    \n",
    "    RECIBE: Un texto plano que puede tener cualquier cosa\n",
    "    DEVUELVE: Una lista de todos los strings que estaban precedidos por un simbolo(sea @, #, etc)\n",
    "    EJEMPLO: 'Hola que tal soy @axel, y vivo en @capitalFederal' -----> [axel,capitalFederal]'\n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    users = []\n",
    "    userToSave = \"\"\n",
    "    for word in words:\n",
    "        if len(word) != 0 and symbol in word:\n",
    "            splittedWord = word.split(symbol)\n",
    "            if len(splittedWord) > 1:\n",
    "                userToSave = splittedWord[1]\n",
    "            else:\n",
    "                userToSave = splittedWord[0]\n",
    "        if userToSave != '':\n",
    "            users.append(userToSave)\n",
    "            userToSave = \"\"\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeDots(word):\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra cualquiera\n",
    "    DEVUELVE: La parte de la palabra que no tiene ningun punto\n",
    "    EJEMPLO: 'esperando...' -----> 'esperando'\n",
    "    \"\"\"\n",
    "    \n",
    "    splited = word.split('.')\n",
    "    \n",
    "    for chunk in splited:\n",
    "        if chunk != '.':\n",
    "            return chunk\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasNumeric(word):\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra cualquiera\n",
    "    DEVUELVE: True o False dependiendo de si contiene algun numero\n",
    "    EJEMPLO: 'hol4' -----> True\n",
    "    \"\"\"    \n",
    "    \n",
    "    for c in word:\n",
    "        if c.isnumeric():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getEnlgishWord(word):\n",
    "       \n",
    "    \"\"\"\n",
    "    RECIBE: Un texto plano que puede tener cualquier cosa\n",
    "    DEVUELVE: La una tupla con la palabra mas larga en ingles que encuentre primero dentro del texto recibido\n",
    "              y un True o False dependiendo de si habia o no una palabra en ingles en el texto. \n",
    "              Si no contiene ninguna palabra en ingles devuelve el texto original.\n",
    "    TENER EN CUENTA: No considera como una palabra valida a las que tengan longitud 1\n",
    "    EJEMPLO: 'aaaaaaaaaaENGLISHaaaaaaaaaa' -----> 'ENGLISH'\n",
    "    EJEMPLO: 'aaaaaaaaaaaaaaaaaaaa' -----> 'aaaaaaaaaaaaaaaaaaaa'\n",
    "    \"\"\"  \n",
    "    \n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    MIN_LEN_WORDS = 2\n",
    "    \n",
    "    failure = False\n",
    "    \n",
    "    if word == None:\n",
    "        failure = True\n",
    "    \n",
    "    elif len(word) < MIN_LEN_WORDS:\n",
    "        failure = True\n",
    "        \n",
    "    else:\n",
    "        for i in range(len(word), MIN_LEN_WORDS - 1,-1):\n",
    "            for j in range(i):\n",
    "                aux = word[j:i]\n",
    "                if englishDictionary.check(aux) and not hasNumeric(aux) and len(aux) > 1:\n",
    "                    return removeDots(aux), True\n",
    "                else:\n",
    "                    failure = True\n",
    "    \n",
    "    if failure:  \n",
    "        return word, False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapBinaryLabel(binaryIndicator, labelsList):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: 1 o 0\n",
    "    DEVUELVE: labelsList[0] o labelsList[1] respectivamente\n",
    "    \"\"\"  \n",
    "    \n",
    "    if binaryIndicator == 1:\n",
    "        return labelsList[0]\n",
    "    if binaryIndicator == 0:\n",
    "        return labelsList[1]\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateAbsolutePath(fileName, exists = False, nombreRepo = 'DatosRepo'):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo\n",
    "            exists es para indicar que el archivo ya existe. En cuyo caso lo va a buscar a donde este\n",
    "    DEVUELVE: El path absoluto del archivo\n",
    "    TENER EN CUENTA: Si se pone mal escrito el archivo es posible que lo encuentre igual por ej 'trai' en vez de 'train.csv'\n",
    "                     Asume que tu repo se llama DatosRepo a menos que se lo digas en <nombreRepo>\n",
    "    EJEMPLO: 'miArchivo.txt' -----> '/home/axelmpm/DatosRepo/miArchivo.txt'\n",
    "    \"\"\"\n",
    "    \n",
    "    path = '/home/' + getpass.getuser() + '/' + nombreRepo\n",
    "    \n",
    "    if exists:\n",
    "        for dirpath, dirnames, filenames in os.walk(path):\n",
    "            for filename in [f for f in filenames if (os.path.basename(f) == fileName)]:\n",
    "                return os.path.join(dirpath, filename)\n",
    "    else:\n",
    "        return os.path.abspath(\"\") + '/' + fileName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readDic(fileName, transformStringedListToList = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo .csv\n",
    "    DEVUELVE: El diccionario que estaba almacenado en disco\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(generateAbsolutePath(fileName + '.csv'))\n",
    "    \n",
    "    if transformStringedListToList:\n",
    "        df['values'] = df.apply(lambda row : ast.literal_eval(row['values']), axis = 1)\n",
    "    \n",
    "    dic = {}\n",
    "    \n",
    "    keys = list(df['keys'])\n",
    "    values = list(df['values'])\n",
    "    \n",
    "    for i in range(len(keys)):\n",
    "        dic[keys[i]] = values[i]\n",
    "    \n",
    "    return dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordInListFromFile(fileName):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El nombre de un archivo donde cada linea tiene palabras\n",
    "    DEVUELVE: Una lista donde cada elemento son las palabras que estaban como linea en el archivo (no toma las palabras de longitud 1)\n",
    "    \"\"\" \n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'r')\n",
    "    \n",
    "    l = []\n",
    "    \n",
    "    for line in file:\n",
    "        if len(line[:-1]) > 1:\n",
    "            l.append(line[:-1]) #le saco el \\n\n",
    "    file.close()\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasCertainWords(stuffWithWordsInIt,words):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Recibe algo que tiene palabras dentro (texto, lista de palabras, conjuntos con palabras, etc)\n",
    "    DEVUELVE: Devuelve True o False dependiendo de si el el contendor contiene ALGUNA palabra en la lista de palabras\n",
    "    \"\"\" \n",
    "    \n",
    "    for word in words:\n",
    "        if word in stuffWithWordsInIt:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clasifyWordsRespectToEnglish(bagOfWords):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un contenedor de palabras que podrian o no estar en ingles\n",
    "    DEVUELVE: Una lista de palabras que estan en ingles y otra que no estaban en ingles\n",
    "    EJEMPLO: {'hello..gggggg,ghghghgh,who,hola'} -------> {hello,who}, {ghghghgh,hola}\n",
    "    \"\"\" \n",
    "    \n",
    "    bagOfEnglishWords = []\n",
    "    bagOfNonEnglishWords = []\n",
    "    for word in bagOfWords:\n",
    "        processedWord,isEnglish = getEnlgishWord(word)\n",
    "        if isEnglish:\n",
    "            bagOfEnglishWords.append(processedWord)\n",
    "        else:\n",
    "            bagOfNonEnglishWords.append(processedWord)\n",
    "        \n",
    "    return bagOfEnglishWords, bagOfNonEnglishWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataFrameOfFrecuencies(elementsLabel,elements, sort = True, sortAscending = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista o secuencia de elementos y un nombre para estos\n",
    "    DEVUELVE: Un dataframe con dos columnas, una de labels y otro de las frecuencias\n",
    "    \"\"\" \n",
    "    \n",
    "    temp_elements = list(elements.copy())\n",
    "    frecs_dict = {i:temp_elements.count(i) for i in temp_elements}\n",
    "\n",
    "    df = pd.DataFrame({elementsLabel : list(frecs_dict.keys()), 'Cantidad' : list(frecs_dict.values())})\n",
    "    \n",
    "    if sort:\n",
    "        df.sort_values('Cantidad', ascending = sortAscending, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dicToDataFrame(dic, keysLabel = \"keys\", valuesLabel = \"values\", sort = True, sortAscending = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un diccionario y nombres para sus claves y valores y si si quiere ordenar o no ascendente o no\n",
    "    DEVUELVE: Un dataframe con dos columnas, una de keys y otro de las values\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.DataFrame({keysLabel : list(dic.keys()), valuesLabel : list(dic.values())})\n",
    "    \n",
    "    if sort:\n",
    "        df.sort_values(valuesLabel, ascending = sortAscending, inplace = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getHammingSimilarity(word1,word2):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos palabras de igual longitud\n",
    "    DEVUELVE: Un numero del 0 al 1 donde 1 significa que son el mismo string y 0 es que no se parecen en nada\n",
    "    EJEMPLO: word1 = casa, word2 = capa ---->  0.75 (porque tienen en comun la primera c, la segunda a\n",
    "             y la ultima a, osea 3/4 bien)\n",
    "    \"\"\"     \n",
    "    \n",
    "    length = len(word1)\n",
    "    mismatches = 0\n",
    "    \n",
    "    for i in range(length):\n",
    "        if word1[i] != word2[i]:\n",
    "            mismatches += 1\n",
    "    return 1 - mismatches/length\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAllConstantSum(k,n):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: k es el valor a sumar de forma constante, n es la cantidad de numeros a sumar\n",
    "    DEVUELVE: Todas las listas de n numeros que sumados dan k \n",
    "    \"\"\" \n",
    "    \n",
    "    if n == 1:\n",
    "        return [[k]]\n",
    "    \n",
    "    sums = []\n",
    "    \n",
    "    for i in range(k + 1):\n",
    "        \n",
    "        subsums = getAllConstantSum(k - i, n -1)\n",
    "        \n",
    "        for subsum in subsums:\n",
    "            sums.append([i] + subsum)\n",
    "    \n",
    "    return sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateWordExtentionsIntoNChars(word,n,extentionChar):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra a extender, la cantidad de caracteres que debe tener la version extendida y que caracter usar al extender\n",
    "    DEVUELVE: Devuelve un set de todas las posibles extensiones\n",
    "    EJEMPLO: word = 'casa', n = 6, extentionChar = '.' ---->\n",
    "                {'..casa','.c.asa','.ca.sa','.cas.a','.casa.','c..asa','c.a.sa','c.as.a','c.asa.','ca..sa'\n",
    "                    'ca.s.a','ca.sa.','cas..a','cas.a.','casa..'}   \n",
    "    \"\"\"\n",
    "    \n",
    "    if len(word) == '':\n",
    "        return [extentionChar * n]\n",
    "    \n",
    "    if (len(word) > n):\n",
    "        return [word]\n",
    "    \n",
    "    if len(word) == n:\n",
    "        return [word]\n",
    "    \n",
    "    wordLength = len(word)\n",
    "    amountOfCharsToAdd = n - wordLength\n",
    "    \n",
    "    extentions = set()\n",
    "    \n",
    "    amounts = getAllConstantSum(amountOfCharsToAdd,wordLength + 1)\n",
    "    \n",
    "    for amountOfExtendingChars in amounts:\n",
    "\n",
    "        extention = ''\n",
    "        for i in range(len(word)):\n",
    "            extention += (extentionChar * amountOfExtendingChars[i]) + word[i]\n",
    "        extention += extentionChar * amountOfExtendingChars[-1]\n",
    "        \n",
    "        extentions.add(''.join(list(extention)))\n",
    "    \n",
    "    return extentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSimilarity(text1,text2):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos textos de cualquier longitud (no muy largos)\n",
    "    DEVUELVE: Un numero del 0 al 1 donde 1 significa que son el mismo string y 0 es que no se parecen en nada\n",
    "    A TENER EN CUENTA: Usa la similaridad de Hamming\n",
    "                       Es mas o menos costosa a partir de 20 caracteres de texto aprox\n",
    "                       Remueve los espacios por lo que no hace falta preprocesar texto\n",
    "    EJEMPLO: word1 = casa, word2 = ca ---->  0.5 (porque tienen en comun la primera c, la segunda a, osea 2/4 bien)\n",
    "    \"\"\" \n",
    "    \n",
    "    EMPTY_CHARACTER = ' '\n",
    "    \n",
    "    if text1 + text2 == '':\n",
    "        return 1\n",
    "    \n",
    "    if text1 + text2 == text1 or text1 + text2 == text2:\n",
    "        return 0\n",
    "    \n",
    "    splitedText1 = text1.split(' ')\n",
    "    splitedText2 = text2.split(' ')\n",
    "    \n",
    "    concatenatedWord1 = (''.join(splitedText1)).lower()\n",
    "    concatenatedWord2 = (''.join(splitedText2)).lower()\n",
    "    \n",
    "    if len(concatenatedWord1) > len(concatenatedWord2):\n",
    "        longest = concatenatedWord1\n",
    "        shortest = concatenatedWord2\n",
    "    else:\n",
    "        longest = concatenatedWord2\n",
    "        shortest = concatenatedWord1\n",
    "        \n",
    "    lenDif = len(longest) - len(shortest)\n",
    "    maxSimilarity = 0\n",
    "    extentions = generateWordExtentionsIntoNChars(shortest,len(longest),EMPTY_CHARACTER)\n",
    "    \n",
    "    for extention in extentions:\n",
    "        \n",
    "        subSimilarity = getHammingSimilarity(extention,longest)\n",
    "        #print(shortest,subWord,longest,subSimilarity)\n",
    "        \n",
    "        if (subSimilarity >= maxSimilarity):\n",
    "            maxSimilarity = subSimilarity\n",
    "            \n",
    "    return maxSimilarity\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMaxSimilarityWord(word,words):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: El una palabra y palabras a comparar\n",
    "    DEVUELVE: La palabra que mas se parece\n",
    "    \"\"\"\n",
    "    \n",
    "    maxSimilarity = 0\n",
    "    mostSimilarKnownWord = ''\n",
    "    \n",
    "    if word in words:\n",
    "        return word\n",
    "    \n",
    "    candidateWords = []\n",
    "    for w in words:\n",
    "        if len(w) > 0 and w[0].lower() == word[0].lower():\n",
    "            candidateWords.append(w)\n",
    "    \n",
    "    for knownWord in candidateWords:\n",
    "        \n",
    "        similarity = getSimilarity(word,knownWord)\n",
    "\n",
    "        if maxSimilarity <= similarity:\n",
    "\n",
    "            maxSimilarity = similarity\n",
    "            mostSimilarKnownWord = knownWord\n",
    "            \n",
    "    return mostSimilarKnownWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isSimilar(text1, text2, threshold = 0.7):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Dos textos (no muy grandes)\n",
    "    DEVUELVE: Si la similitud es mayor a un threshold\n",
    "    \"\"\" \n",
    "    \n",
    "    return getSimilarity(text1,text2) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isIn(element,container):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un elemento y un contenedor de elementos\n",
    "    DEVUELVE: Si el elemento esta o no en el contenedor\n",
    "    \"\"\" \n",
    "    \n",
    "    return element in container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acortar_texto(df):\n",
    "    \"\"\"\n",
    "    RECIBE: un df\n",
    "    DEVUELVE: el mismo df con una columna con version corta de la columna 'text'\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df['short_text'] = 'None'\n",
    "    cant_filas = len(df.index)\n",
    "    for y in range(cant_filas):\n",
    "        df['short_text'][y:]= df.iloc[y][0][:15] + \"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasQuestion(text):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: True si tiene al menos una palabra en ingles seguida por al menos un signo de pregunta, sino retorna FALSE\n",
    "    '''\n",
    "    text = text.replace('?', ' ? ')\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    text = takeOutSpecialCaractersFromText(text, specialCharacters = [',','.','#','!','%','*','(',')','\"','-','_', '$', ':', '/', '\\''])\n",
    "    textSplitted = text.split()\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            for j in range(textSplitted.index(word) + 1, len(textSplitted)):\n",
    "                if ('?' in textSplitted[j]):\n",
    "                    return True\n",
    "            return False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def takeOutSpecialCaractersFromText(text, specialCharacters = None):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: El string sin caracteres especiales. Con el parametro specialCharacters se puede decidir que caracteres sacar.\n",
    "    '''\n",
    "    if specialCharacters == None:\n",
    "        specialCharacters = ['&','\\n','<','>','=',',','.','#','!','%','*','(',')','\"','-','_', '?', '$', ':', '/','\\''] + [str(i) for i in range(0,10)]\n",
    "    \n",
    "    \n",
    "    for char in specialCharacters:\n",
    "        text = text.replace(char, ' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separateOutBiaSpecialCaractersAndMayuscFromText(text, specialCharacters = None):\n",
    "    '''\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: Una lista de los substrings que fuerons separados por los caracteres especiales o las mayusuclas.\n",
    "               Con el parametro specialCharacters se puede decidir que caracteres sacar.\n",
    "    '''\n",
    "    if specialCharacters == None:\n",
    "        specialCharacters = ['&','\\n','<','>','=',',','.','#','!','%','*','(',')',\n",
    "                             '\"','-','_', '?', '$', ':', '/','\\''] + [str(i) for i in range(0,10)]\n",
    "    \n",
    "    words = []\n",
    "    subWord = ''\n",
    "    prevC = ''\n",
    "    \n",
    "    for c in text:\n",
    "        \n",
    "        if c.isupper() and prevC.islower():\n",
    "            words.append(subWord)\n",
    "            subWord = c\n",
    "        else:        \n",
    "            if c in specialCharacters:\n",
    "                if subWord != '':\n",
    "                    words.append(subWord)\n",
    "                    subWord = ''\n",
    "            else:\n",
    "                subWord += c\n",
    "        prevC = c\n",
    "        \n",
    "    if subWord != '':\n",
    "        words.append(subWord)\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def barplot(x,y,data,xLabel = None,yLabel = None,title = '',\n",
    "            palette = 'Spectral',figX = 25,figY = 10,fontSize = 1.7):\n",
    "    \n",
    "    '''\n",
    "    RECIBE:\n",
    "        x: es el label de la columna con los datos de x del dataFrame 'data'\n",
    "        y: es el label de la columna con los datos de y del dataFrame 'data'\n",
    "        data: es el dataframe de donde sacar los datos\n",
    "        xLabel: es el label para x en el plot\n",
    "        yLabel: es el label para y en el plot\n",
    "        xLabel: es el titulo del plot\n",
    "        palette: es la paleta de colores a usar\n",
    "        figX: es el tamaño en x del plot\n",
    "        figY: es el tamaño en y del plot\n",
    "        fontSize: es el tamaño del font en el plot\n",
    "    A TENER EN CUENTA: No afecta al dataframe 'data'\n",
    "    DEVUELVE: El plot\n",
    "    '''\n",
    "    \n",
    "    if xLabel == None:\n",
    "        xLabel = x\n",
    "    \n",
    "    if yLabel == None:\n",
    "        yLabel = y\n",
    "    \n",
    "    sns.set(rc = {'figure.figsize':(figX,figY)})\n",
    "    sns.set(font_scale = fontSize)\n",
    "    v = sns.barplot(x = x, y = y, data = data, \n",
    "                 palette = palette, saturation = 1)\n",
    "    \n",
    "    v.set(xlabel = xLabel, ylabel= yLabel)\n",
    "    v.set(title = title)\n",
    "    \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addContextualKnowledge(wordList,context):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de palabras y un contexto de textos previos\n",
    "    DEVUELVE: El contexto actualizado con el contenido del texto pasado\n",
    "    \"\"\"\n",
    "    \n",
    "    words = list(set(wordList))\n",
    "    pairs = []\n",
    "    \n",
    "    if len(words) == 1:\n",
    "        pairs.append((words[0],words[0]))\n",
    "    \n",
    "    else:\n",
    "        for i in range(len(words)):\n",
    "            for j in range(i + 1,len(words)):\n",
    "                if j < len(words):\n",
    "                    pairs.append((words[i],words[j]))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        \n",
    "        if pair in context:\n",
    "            context[pair] += 1\n",
    "        else:\n",
    "            context[pair] = 1\n",
    "            \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualKnowledge(texts):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de listas de palabras (lista de tweets)\n",
    "    DEVUELVE: Un diccionario llamado contexto que tiene como claves combinaciones de palabras que se dieron\n",
    "                juntas en algun texto de la lista y cuya clave es la cantidad de textos en los que se dio\n",
    "    \"\"\"\n",
    "    \n",
    "    context = {}\n",
    "    \n",
    "    for wordList in texts:\n",
    "        \n",
    "        context = addContextualKnowledge(wordList,context)\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPairsWith(word,context):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra y un contexto\n",
    "    DEVUELVE: Devuelve los pares de palabras del contexto que contienen a la palabra pasada\n",
    "    \"\"\"\n",
    "    \n",
    "    pairsWith = []\n",
    "    \n",
    "    for pair in context:\n",
    "        \n",
    "        if word in pair:\n",
    "            pairsWith.append(pair)\n",
    "    \n",
    "    return pairsWith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAsociatedWordsWithReinforcement(word,context):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra y un contexto\n",
    "    DEVUELVE: Una lista de (palabra,reinforcements) que seria como las palabras y la frecuencia con la que\n",
    "                las vio juntos a word y a palabra\n",
    "    \"\"\"\n",
    "    \n",
    "    asociatedPairs = getPairsWith(word,context)\n",
    "    asociatedWordAndReinforcement = []\n",
    "\n",
    "    for asociatedPair in asociatedPairs:\n",
    "        \n",
    "        if word != asociatedPair[0]:\n",
    "            asociatedWord = asociatedPair[0]\n",
    "        else:\n",
    "            asociatedWord = asociatedPair[1]\n",
    "\n",
    "        asociatedWordAndReinforcement.append((asociatedWord,context[asociatedPair]))\n",
    "  \n",
    "    return asociatedWordAndReinforcement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualMap(context):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un contexto\n",
    "    DEVUELVE: Un diccionario que contiene como clave todas las palabras del contexto y como value sus asociadas\n",
    "                con su reinforcement\n",
    "    \"\"\"\n",
    "    \n",
    "    contextualMap = {}\n",
    "    \n",
    "    for pair in context:\n",
    "        \n",
    "        for word in pair:\n",
    "            \n",
    "            if not word in contextualMap:\n",
    "\n",
    "                asociatedWordAndReinforcement = getAsociatedWordsWithReinforcement(word,context)\n",
    "\n",
    "                contextualMap[word] = asociatedWordAndReinforcement\n",
    "            \n",
    "    return contextualMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeIntoWords(texts):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de lista de palabras\n",
    "    DEVUELVE: Una lista de las palabras de los textos recibidos\n",
    "    \"\"\"\n",
    "    return list(set(itertools.chain.from_iterable(texts)))                \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanUpWords(texts):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de textos\n",
    "    DEVUELVE: Una lista de lista de palabras (lista de tweets) cuyas palabras de los textos recibidos no son vacias\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanedTexts = []\n",
    "    \n",
    "    stopWords =  set(stopwords.words('english'))\n",
    "    \n",
    "    for text in texts:\n",
    "    \n",
    "        words = text.split(' ')\n",
    "        cleanedWords = []\n",
    "        \n",
    "        for word in words:\n",
    "            \n",
    "            subWords = separateOutBiaSpecialCaractersAndMayuscFromText(word) \n",
    "            \n",
    "            for subWord in subWords:\n",
    "                \n",
    "                if subWord == '':\n",
    "                    continue\n",
    "                else:\n",
    "                    cleanedWords.append(subWord.lower())\n",
    "        \n",
    "        cleanedTexts.append(cleanedWords)\n",
    "        \n",
    "    return cleanedTexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def furtherCleanUp(words,stopWords):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Una lista de palabras y unas stopwords\n",
    "    DEVUELVE: Una lista de palabras que no tienen longitud menor a 3 ni son stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    cleanWords = []\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if not (len(word) < 3 or word in stopWords):\n",
    "            cleanWords.append(word)\n",
    "            \n",
    "    return cleanWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncateMapTo(contextualMap,words):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Toma un contextual map y unas palabras\n",
    "    DEVUELVE: Un contextual map cuyas palabras en key y en los values no estan en la lista de palabras recibidas\n",
    "    \"\"\"\n",
    "    \n",
    "    truncatedContextualMap = {}\n",
    "    \n",
    "    for word in contextualMap:\n",
    "        \n",
    "        if word in words:\n",
    "            asociates = contextualMap[word]\n",
    "\n",
    "            filteredAsociates = []\n",
    "\n",
    "            i = 0\n",
    "            checksForEveryWordInAsociates = [pair[0] in words for pair in asociates]\n",
    "\n",
    "            for asociatedWordIsIn in checksForEveryWordInAsociates:\n",
    "\n",
    "                if asociatedWordIsIn:\n",
    "                    filteredAsociates.append(asociates[i])\n",
    "                i += 1\n",
    "\n",
    "            truncatedContextualMap[word] = filteredAsociates\n",
    "    return truncatedContextualMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFromFileIntoList(fileName, strings = False):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un nombre de archivo que se supone que almacena una lista previamente almacenada \n",
    "            con el metodo listToFile\n",
    "    DEVUELVE: La lista guardada en ese archivo\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'r')\n",
    "    \n",
    "    strList = (file.read()).split('&&&')[:-1]\n",
    "    \n",
    "    recoveredList = []\n",
    "    \n",
    "    for e in strList:\n",
    "        \n",
    "        if strings:\n",
    "            recoveredList.append(e)\n",
    "        else:\n",
    "            recoveredList.append(ast.literal_eval(e))\n",
    "        \n",
    "    return recoveredList\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToFile(aList,fileName):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: Un nombre para el archivo que va a almacenar la lista y \n",
    "            una lista a almacenar\n",
    "    \"\"\"\n",
    "    \n",
    "    file = open(generateAbsolutePath(fileName),'w')\n",
    "    \n",
    "    for e in aList:\n",
    "        file.write(str(e) + '&&&')\n",
    "        \n",
    "    file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractTextFrom(data):\n",
    "\n",
    "    \"\"\" \n",
    "    RECIBE: El data frame de tweets\n",
    "    TENER EN CUENTA: \n",
    "            Considera el campo location, keyword y text\n",
    "            No altera al data frame\n",
    "    DEVUELVE: En una lista como elementos la concatenacion de campo keyword, location y text\n",
    "    \"\"\"\n",
    "    data_copy = data.copy()\n",
    "    data_copy['keyword'].fillna('', inplace = True)\n",
    "    data_copy['location'].fillna('', inplace = True)\n",
    "    \n",
    "    return list(data_copy['keyword'] + ' ' + data_copy['location'] + ' ' + data_copy['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mergeTextIn(row):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una row de data frame de tweets\n",
    "    TENER EN CUENTA: \n",
    "            Considera el campo location, keyword y text\n",
    "            No altera al data frame\n",
    "    DEVUELVE: La concatenacion de campo keyword, location y text\n",
    "    \"\"\"\n",
    "    \n",
    "    return row['keyword'] + ' ' + row['location'] + ' ' + row['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractNounFrequencies(data,nouns):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: El data frame de tweets y una lista de nouns\n",
    "    TENER EN CUENTA: \n",
    "            Considera el campo location, keyword y text\n",
    "            No altera al data frame\n",
    "    DEVUELVE: Un diccionario de frecuencia de nouns en el dataframe y otro de frecuencia en desastres\n",
    "    \"\"\"\n",
    "    \n",
    "    data_copy = data.copy()\n",
    "    data_copy['keyword'].fillna('', inplace = True)\n",
    "    data_copy['location'].fillna('', inplace = True)\n",
    "    \n",
    "    disasterNounFrequency = {}\n",
    "    totalNounFrequency = {}\n",
    "    \n",
    "    for index, row in data_copy.iterrows():\n",
    "    \n",
    "        text = mergeTextIn(row)\n",
    "\n",
    "        for word in nouns:\n",
    "\n",
    "            if word in text:\n",
    "\n",
    "                if word in totalNounFrequency:\n",
    "                    totalNounFrequency[word] += text.count(word)\n",
    "\n",
    "                    if row['target'] == 1:\n",
    "\n",
    "                        if word in disasterNounFrequency:\n",
    "                            disasterNounFrequency[word] += text.count(word)\n",
    "\n",
    "                        else:\n",
    "                            disasterNounFrequency[word] = text.count(word)\n",
    "\n",
    "                else:\n",
    "                    totalNounFrequency[word] = text.count(word)\n",
    "                    \n",
    "    return disasterNounFrequency,totalNounFrequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWordIntrinsicNegativity(word,disasterNounFrequency,totalNounFrequency):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra y los mapas de frecuencia en desastre y total\n",
    "    DEVUELVE: La frecuencia de uso de word en desastres / la frecuencia de word en todo el data frame\n",
    "    \"\"\"\n",
    "    \n",
    "    if word in disasterNounFrequency and word in totalNounFrequency:\n",
    "        return disasterNounFrequency[word] / totalNounFrequency[word]\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateNegativityMap(nouns,disasterNounFrequency,totalNounFrequency):\n",
    "   \n",
    "    \"\"\" \n",
    "    RECIBE: El data frame de tweets y los mapas de frecuencia en desastre y total\n",
    "    DEVUELVE: Un diccionario que tiene como key nouns y value su negatividad\n",
    "    \"\"\"\n",
    "    nounNegativityMap = {}\n",
    "    \n",
    "    for noun in nouns:\n",
    "        \n",
    "        nounNegativityMap[noun] = getWordIntrinsicNegativity(noun,disasterNounFrequency,totalNounFrequency)\n",
    "        \n",
    "    return nounNegativityMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isAsociated(word1,word2,contextualMap):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Dos palabras y un contexto\n",
    "    DEVUELVE: Si la palabra 1 esta en el conjunto de palabras asociadas a la palabra 2\n",
    "    \"\"\"\n",
    "    \n",
    "    if not word2 in contextualMap:\n",
    "        return False\n",
    "\n",
    "    for pair in contextualMap[word2]:\n",
    "        \n",
    "        if pair[0] == word1:\n",
    "            return True\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getContextualNegativity(word,words,context):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Una palabra, una lista de palabras y un contexto\n",
    "    DEVUELVE: La negatividad contextual por la relacion entre estas palabras\n",
    "    \"\"\"\n",
    "    \n",
    "    contextualNegativity = 0\n",
    "    \n",
    "    for w in words:\n",
    "        \n",
    "        if isAsociated(w,word,context.englishContextualMap) and w != word:\n",
    "            \n",
    "            reinforcement = [asociate[1] for asociate in context.englishContextualMap[w] if asociate[0] == word][0]\n",
    "            totalReinforcementMass = sum([asociate[1] for asociate in context.englishContextualMap[w]])\n",
    "            \n",
    "            contextualNegativity += reinforcement/totalReinforcementMass\n",
    "            \n",
    "    return contextualNegativity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTextNegativity(text,context):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un texto y un context\n",
    "    DEVUELVE: Un numero de 0 a 1 donde 1 es completamente negativo y 0 es nada negativo\n",
    "    \"\"\"\n",
    "    \n",
    "    words = text.split(' ')\n",
    "    \n",
    "    cleanedUpWords = []\n",
    "    \n",
    "    for word in words:\n",
    "        \n",
    "        if word != '':\n",
    "            mostSimilar = getMaxSimilarityWord(word,context.nouns)\n",
    "            \n",
    "            if mostSimilar != '':\n",
    "                cleanedUpWords.append(mostSimilar)\n",
    "    \n",
    "    intrinsicNegativity = 0\n",
    "    contextualNegativity = 0\n",
    "    totalIntrinsicNegativity = 0\n",
    "    totalContextualNegativity = 0\n",
    "    contextualPonderation = 0\n",
    "    \n",
    "    for word in cleanedUpWords: \n",
    "        \n",
    "        intrinsicNegativity = context.nounNegativityMap[word]\n",
    "        contextualNegativity =  getContextualNegativity(word,cleanedUpWords,context)\n",
    "        \n",
    "        if contextualNegativity > 0:\n",
    "            contextualPonderation += 1\n",
    "            \n",
    "        totalIntrinsicNegativity += intrinsicNegativity\n",
    "        totalContextualNegativity += contextualNegativity * intrinsicNegativity\n",
    "    \n",
    "    #print(\"TOTAL NEGATIVITY: {}, NEGATIVITY EXTRACTED FROM CONTEXT: {}\".format(totalIntrinsicNegativity + totalContextualNegativity * contextualPonderation,totalContextualNegativity * contextualPonderation))\n",
    "    return totalIntrinsicNegativity + totalContextualNegativity * contextualPonderation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Context:\n",
    "    \n",
    "    \"\"\" \n",
    "    Clase que engloba toda las operaciones y nociones de contexto de un texto\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,data = None, similarityThreshold = 0.8, load = ''):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: \n",
    "            texts es una lista de textos (tweets) a entender\n",
    "            similarityThreshold es una tolerancia para la similaridad. \n",
    "                                Si dos palabras superan ese limite se consideran simialres\n",
    "            load es opcional y es el nombre del archivo que almacena\n",
    "                                el contextualMap guardado en disco en caso de haberlo hecho\n",
    "        \"\"\"\n",
    "        \n",
    "        self.stopWords = set(stopwords.words('english'))\n",
    "        self.similarityThreshold = similarityThreshold\n",
    "        \n",
    "        self.__atrbt_texts = load + '_' + 'texts'\n",
    "        self.__atrbt_words = load + '_' + 'words'\n",
    "        self.__atrbt_englishWords = load + '_' + 'englishWords'\n",
    "        self.__atrbt_nonEnglishWords = load + '_' + 'nonEnglishWords'\n",
    "        self.__atrbt_nouns = load + '_' + 'nouns'\n",
    "        \n",
    "        self.__atrbt_wordNet = load + '_' + 'wordNet'\n",
    "        self.__atrbt_disasterNounFrequency = load + '_' + 'disasterNounFrequency'\n",
    "        self.__atrbt_totalNounFrequency = load + '_' + 'totalNounFrequency'\n",
    "        self.__atrbt_nounNegativityMap = load + '_' + 'nounNegativityMap'\n",
    "        self.__atrbt_contextualMap = load + '_' + 'contextualMap'\n",
    "        self.__atrbt_englishContextualMap = load + '_' + 'englishContextualMap'\n",
    "            \n",
    "            \n",
    "        #====================================================\n",
    "        self.data = data\n",
    "        print(\"Data loaded\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_texts):\n",
    "            self.texts = readFromFileIntoList(self.__atrbt_texts)\n",
    "            print(\"Texts loaded\")\n",
    "        else:\n",
    "            self.texts = cleanUpWords(extractTextFrom(data))\n",
    "            print(\"Extracted texts\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_words):\n",
    "            self.words = readFromFileIntoList(self.__atrbt_words, strings = True)\n",
    "            print(\"Words loaded\")\n",
    "        else:\n",
    "            self.words = mergeIntoWords(self.texts)\n",
    "            print(\"Extracted words (without weird characters, no empty words and all to lower case)\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_englishWords) and path.exists(self.__atrbt_nonEnglishWords):\n",
    "            \n",
    "            self.englishWords = readFromFileIntoList(self.__atrbt_englishWords, strings = True)\n",
    "            print(\"English Words loaded\")\n",
    "            \n",
    "            self.nonEnglishWords = readFromFileIntoList(self.__atrbt_nonEnglishWords, strings = True)\n",
    "            print(\"Non English Words loaded\")\n",
    "        else:\n",
    "            self.englishWords, self.nonEnglishWords = clasifyWordsRespectToEnglish(self.words)\n",
    "            print(\"Words clasified into enlgish and non english\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_nouns):\n",
    "            self.nouns = readFromFileIntoList(self.__atrbt_nouns, strings = True)\n",
    "            print(\"Nouns loaded\")\n",
    "        else:\n",
    "            self.nouns = furtherCleanUp(self.englishWords,self.stopWords)\n",
    "            print(\"Extracted nouns\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_wordNet + '.csv'):\n",
    "            self.wordNet = readDic(self.__atrbt_wordNet)\n",
    "            print(\"Word Net loaded\")\n",
    "        else:\n",
    "            self.wordNet = getContextualKnowledge(self.texts)\n",
    "            print(\"Word Net produced\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_disasterNounFrequency + '.csv') and path.exists(self.__atrbt_totalNounFrequency + '.csv'):\n",
    "            \n",
    "            self.disasterNounFrequency = readDic(self.__atrbt_nounNegativityMap)\n",
    "            print(\"Disaster Frequency map loaded\")\n",
    "            \n",
    "            self.totalNounFrequency = readDic(self.__atrbt_nounNegativityMap)\n",
    "            print(\"Total Frequency map loaded\")\n",
    "        else:\n",
    "            self.disasterNounFrequency, self.totalNounFrequency = extractNounFrequencies(self.data,self.nouns)\n",
    "            print(\"Total and Disaster frequencies calculated for every noun\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_nounNegativityMap + '.csv'):\n",
    "            self.nounNegativityMap = readDic(self.__atrbt_nounNegativityMap)\n",
    "            print(\"Noun Negativity map loaded\")\n",
    "        else:\n",
    "            self.nounNegativityMap = calculateNegativityMap(self.nouns,self.disasterNounFrequency,self.totalNounFrequency)\n",
    "            print(\"Negativity map produced\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_contextualMap + '.csv'):\n",
    "            self.contextualMap = readDic(self.__atrbt_contextualMap, transformStringedListToList = True)\n",
    "            print(\"Contextual map loaded\")\n",
    "        else:\n",
    "            self.contextualMap = getContextualMap(self.wordNet)\n",
    "            print(\"Contextual map produced\")\n",
    "        \n",
    "        #====================================================\n",
    "        if path.exists(self.__atrbt_englishContextualMap + '.csv'):\n",
    "            self.englishContextualMap = readDic(self.__atrbt_englishContextualMap, transformStringedListToList = True)  \n",
    "            print(\"English Truncated Contextual map loaded\")\n",
    "        else:\n",
    "            self.englishContextualMap = truncateMapTo(self.contextualMap,self.nouns)\n",
    "            print(\"Enlgish truncated contextual map produced\")\n",
    "        \n",
    "        \n",
    "    def related(self, word, r = 1, restrict = True):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: La palabra de la que se quiere conocer sus asociados y el orden minimo de su asociacion (su reinforcement)\n",
    "                restric es para quedarse solo con las relaciones con palabras filtradas\n",
    "        DEVUELVE: Una lista de todos los asociados (palabras, orden) que cumplen con el r\n",
    "        \"\"\"\n",
    "        \n",
    "        if restrict:\n",
    "            asociated = self.englishContextualMap[getMaxSimilarityWord(word,self.words)]\n",
    "        else:\n",
    "            asociated = self.contextualMap[getMaxSimilarityWord(word,self.words)]\n",
    "        \n",
    "        if r == 1:\n",
    "            return asociated\n",
    "        \n",
    "        strongAsociated = []\n",
    "        \n",
    "        for pair in asociated:\n",
    "            if pair[1] >= r:\n",
    "                strongAsociated.append(pair)\n",
    "                                \n",
    "        return strongAsociated\n",
    "    \n",
    "    \n",
    "    def save(self,name):\n",
    "        \n",
    "        \"\"\" \n",
    "        RECIBE: El nombre de archivo que guarda el contextualMap\n",
    "        DEVUELVE: Guarda el contextualMap en un csv\n",
    "        \"\"\"\n",
    "        \n",
    "        if name == '':\n",
    "            return None\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_texts):\n",
    "            \n",
    "            listToFile(self.texts, name + '_' + 'texts')\n",
    "            print(\"Texts saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_words):\n",
    "            \n",
    "            listToFile(self.words, name + '_' + 'words') \n",
    "            print(\"Words saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_englishWords):\n",
    "            \n",
    "            listToFile(self.englishWords, name + '_' + 'englishWords')\n",
    "            print(\"English Words saved\")\n",
    "\n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_nonEnglishWords):\n",
    "            \n",
    "            listToFile(self.nonEnglishWords, name + '_' + 'nonEnglishWords')\n",
    "            print(\"Non English Words saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_nouns):\n",
    "            \n",
    "            listToFile(self.nouns, name + '_' + 'nouns')\n",
    "            print(\"Nouns saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_wordNet + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.wordNet).to_csv(name + '_' + 'wordNet' + '.csv')\n",
    "            print(\"Word Net saved\")\n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_disasterNounFrequency + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.disasterNounFrequency).to_csv(name + '_' + 'disasterNounFrequency' + '.csv')\n",
    "            print(\"Disaster Frequency map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_totalNounFrequency + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.totalNounFrequency).to_csv(name + '_' + 'totalNounFrequency' + '.csv')\n",
    "            print(\"Total Frequency map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_nounNegativityMap + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.nounNegativityMap).to_csv(name + '_' + 'nounNegativityMap' + '.csv')\n",
    "            print(\"Noun Negativity map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_contextualMap + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.contextualMap).to_csv(name + '_' + 'contextualMap' + '.csv')\n",
    "            print(\"Contextual map saved\")\n",
    "        \n",
    "        #====================================================\n",
    "        if not path.exists(self.__atrbt_englishContextualMap + '.csv'):\n",
    "            \n",
    "            dicToDataFrame(self.englishContextualMap).to_csv(name + '_' + 'englishContextualMap' + '.csv')  \n",
    "            print(\"English Truncated Contextual map saved\")\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probabilityOfGivenThat(eventA,eventB,data):\n",
    "    \n",
    "    \"\"\" \n",
    "    RECIBE: Un data frame y dos funciones booleanas que dado una row del data frame deciden corresponden\n",
    "            o no a los eventos o condiciones A y B\n",
    "    DEVUELVE: Devuelve la probabilidad condicional P(A|B) tomando como espacio muestral equiprobable el\n",
    "              dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    frequencyOfAandB = data[ data.apply(lambda row : eventA(row) and eventB(row), axis = 1)]['id'].count()\n",
    "    frequencyOfB = data[ data.apply(lambda row : eventB(row), axis = 1)]['id'].count()\n",
    "    \n",
    "    return frequencyOfAandB/frequencyOfB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hasEnglishWord(text):\n",
    "    \"\"\"\n",
    "    RECIBE: Un string.\n",
    "    DEVUELVE: True si tiene al menos una palabra en ingles, sino retorna FALSE\n",
    "    \"\"\"\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    text = takeOutSpecialCaractersFromText(text)\n",
    "    textSplitted = text.split()\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def englishPercentage(text):\n",
    "    text = text.lower()\n",
    "    text = takeOutSpecialCaractersFromText(text)\n",
    "    englishDictionary = enchant.Dict(\"en_US\")\n",
    "    textSplitted = text.split()\n",
    "    cantidadDePalabrasEnIngles = 0\n",
    "    for word in textSplitted:\n",
    "        if englishDictionary.check(word):\n",
    "            cantidadDePalabrasEnIngles += 1\n",
    "    return (cantidadDePalabrasEnIngles / len(textSplitted)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
